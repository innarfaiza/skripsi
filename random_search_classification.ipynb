{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM SEARCH HYPERPARAMETER OPTIMIZATION\n",
    "## Model Klasifikasi Aflatoksin dengan ResNet-50\n",
    "\n",
    "### Referensi Ilmiah:\n",
    "1. **Bergstra & Bengio (2012)** - \"Random Search for Hyper-Parameter Optimization\"\n",
    "2. **Yang & Shami (2020)** - \"On Hyperparameter Optimization of Machine Learning Algorithms\"\n",
    "3. **Yu & Zhu (2020)** - \"Hyper-Parameter Optimization: A Review of Algorithms and Applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPRODUCIBILITY\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KONFIGURASI\n",
    "BASE_DIR = pathlib.Path(\".\")\n",
    "DATA_DIR = BASE_DIR / 'dataset_final'  # Sesuaikan dengan folder Anda\n",
    "RESULTS_DIR = BASE_DIR / 'random_search_results'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Jumlah trial random search\n",
    "N_TRIALS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH SPACE DEFINITION\n",
    "\n",
    "Berdasarkan literatur ilmiah:\n",
    "\n",
    "| Parameter | Range | Referensi |\n",
    "|-----------|-------|----------|\n",
    "| LR Phase 1 | [1e-5, 1e-3] | Kornblith et al. (2019) |\n",
    "| LR Phase 2 | [1e-6, 1e-4] | Howard & Ruder (2018) |\n",
    "| Dropout | [0.2, 0.5] | Srivastava et al. (2014) |\n",
    "| Dense Units | [128, 1024] | He et al. (2016) |\n",
    "| Fine-tune Layer | [100, 170] | Raghu et al. (2019) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEARCH SPACE\n",
    "SEARCH_SPACE = {\n",
    "    'lrp1': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3],\n",
    "    'lrp2': [1e-6, 5e-6, 1e-5, 5e-5, 1e-4],\n",
    "    'layer_options': [\n",
    "    [256],               # 1 layer:  256\n",
    "    [512, 256],          # 2 layers: 512 ‚Üí 256\n",
    "    [256, 128],          # 2 layers: 256 ‚Üí 128\n",
    "    [512, 256, 64]],     # 3 layers: 512 ‚Üí 256 ‚Üí 64]\n",
    "    'dropout_rate': [0.3, 0.4, 0.5, 0.6],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'epochs_p1': [10, 15, 20, 25, 30, 35],\n",
    "    'epochs_p2': [20, 30, 40, 50],\n",
    "    'fine_tune_at': [140, 150, 160],\n",
    "}\n",
    "\n",
    "print(\"Search Space:\")\n",
    "for key, values in SEARCH_SPACE.items():\n",
    "    print(f\"  {key}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def sample_hyperparameters(search_space, seed=None):\n",
    "    \"\"\"Random sampling dari search space.\"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    params = {}\n",
    "    for key, values in search_space.items():\n",
    "        params[key] = random.choice(values)\n",
    "    \n",
    "    # Constraint: lrp2 <= lrp1\n",
    "    if params['lrp2'] > params['lrp1']:\n",
    "        params['lrp2'] = params['lrp1'] / 10\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_optimizer(name, learning_rate):\n",
    "    \"\"\"Create optimizer instance.\"\"\"\n",
    "    if name == 'adam':\n",
    "        return keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif name == 'adamw':\n",
    "        return keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n",
    "    elif name == 'sgd':\n",
    "        return keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif name == 'rmsprop':\n",
    "        return keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    return keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Test sampling - verifikasi bahwa arsitektur selalu terurut\n",
    "print(\"\\nContoh sampling (verifikasi urutan):\")\n",
    "for i in range(6):\n",
    "    p = sample_hyperparameters(SEARCH_SPACE, seed=100+i)\n",
    "    arch = \" ‚Üí \".join(map(str, p['layer_options']))\n",
    "    print(f\"  Sample {i+1}: [{arch}] ‚úì Guaranteed descending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / 'train', labels=\"inferred\", label_mode=\"int\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / 'val', labels=\"inferred\", label_mode=\"int\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / 'test', labels=\"inferred\", label_mode=\"int\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Augmentation & Preprocessing\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.028),\n",
    "])\n",
    "\n",
    "def preprocess_train(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = data_augmentation(images, training=True)\n",
    "    images = preprocess_input(images)\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_val(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = preprocess_input(images)\n",
    "    return images, labels\n",
    "\n",
    "train_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE).cache().shuffle(1000).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess_val, num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess_val, num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)\n",
    "\n",
    "print(f\"‚úì Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD MODEL FUNCTION\n",
    "def build_model(params):\n",
    "    \"\"\"Build model dengan hyperparameters.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers berdasarkan layer_options\n",
    "    layer_units = params['layer_options']  # Contoh: [512, 256, 64]\n",
    "    \n",
    "    for i, units in enumerate(layer_units):\n",
    "        x = layers.Dense(units, activation='relu', name=f'dense_{i+1}')(x)\n",
    "        # Dropout setelah setiap layer kecuali layer terakhir\n",
    "        if i < len(layer_units) - 1:\n",
    "            x = layers.Dropout(params['dropout_rate'], name=f'dropout_{i+1}')(x)\n",
    "    \n",
    "    # Jika hanya 1 layer, tambah dropout sebelum output\n",
    "    if len(layer_units) == 1:\n",
    "        x = layers.Dropout(params['dropout_rate'], name='dropout_1')(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SINGLE TRIAL\n",
    "def train_trial(params, trial_id):\n",
    "    \"\"\"Train single trial.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAL {trial_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Format architecture string\n",
    "    layer_units = params['layer_options']\n",
    "    arch_str = \" ‚Üí \".join(map(str, layer_units))\n",
    "    \n",
    "    print(f\"Architecture: [{arch_str}] ({len(layer_units)} layers)\")\n",
    "    print(f\"LR_P1={params['lrp1']}, LR_P2={params['lrp2']}, Dropout={params['dropout_rate']}\")\n",
    "    print(f\"Optimizer={params['optimizer']}, Fine-tune at={params['fine_tune_at']}\")\n",
    "    \n",
    "    model, base_model = build_model(params)\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True, verbose=0),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    # Phase 1: Frozen backbone\n",
    "    print(f\"\\n[Phase 1] Epochs={params['epochs_p1']}\")\n",
    "    model.compile(optimizer=get_optimizer(params['optimizer'], params['lrp1']),\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history_p1 = model.fit(train_ds, validation_data=val_ds, epochs=params['epochs_p1'], callbacks=callbacks, verbose=0)\n",
    "    p1_acc = max(history_p1.history['val_accuracy'])\n",
    "    print(f\"  Phase 1 Val Acc: {p1_acc:.4f}\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning\n",
    "    print(f\"\\n[Phase 2] Epochs={params['epochs_p2']}, Fine-tune from layer {params['fine_tune_at']}\")\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:params['fine_tune_at']]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=get_optimizer(params['optimizer'], params['lrp2']),\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    initial_epoch = len(history_p1.history['loss'])\n",
    "    history_p2 = model.fit(train_ds, validation_data=val_ds, \n",
    "                           initial_epoch=initial_epoch, epochs=initial_epoch + params['epochs_p2'],\n",
    "                           callbacks=callbacks, verbose=0)\n",
    "    p2_acc = max(history_p2.history['val_accuracy'])\n",
    "    print(f\"  Phase 2 Val Acc: {p2_acc:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(model.predict(test_ds, verbose=0), axis=1)\n",
    "    y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\n[Result] Test Accuracy: {test_accuracy:.4f}, F1 Macro: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'trial_id': trial_id,\n",
    "        'architecture': arch_str,\n",
    "        'params': params,\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_loss': float(test_loss),\n",
    "        'f1_macro': float(f1),\n",
    "        'val_acc_p1': float(p1_acc),\n",
    "        'val_acc_p2': float(p2_acc)\n",
    "    }, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN RANDOM SEARCH\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING RANDOM SEARCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = []\n",
    "best_result = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for trial_id in range(1, N_TRIALS + 1):\n",
    "    # Sample hyperparameters\n",
    "    params = sample_hyperparameters(SEARCH_SPACE, seed=42 + trial_id)\n",
    "    \n",
    "    try:\n",
    "        result, model = train_trial(params, trial_id)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save trial\n",
    "        with open(RESULTS_DIR / f'trial_{trial_id:03d}.json', 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        # Check if best\n",
    "        if result['test_accuracy'] > best_accuracy:\n",
    "            best_accuracy = result['test_accuracy']\n",
    "            best_result = result\n",
    "            model.save(RESULTS_DIR / 'best_model.keras')\n",
    "            with open(RESULTS_DIR / 'best_params.json', 'w') as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"\\nüèÜ NEW BEST! Accuracy: {best_accuracy:.4f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial_id} failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nProgress: {trial_id}/{N_TRIALS}, Best so far: {best_accuracy:.4f}\")\n",
    "\n",
    "# Save all results\n",
    "with open(RESULTS_DIR / 'search_history.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM SEARCH COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal trials: {len(all_results)}\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ BEST HYPERPARAMETERS:\")\n",
    "print(\"-\"*40)\n",
    "if best_result:\n",
    "    print(f\"  Architecture: [{best_result['architecture']}]\")\n",
    "    print(f\"  LR Phase 1: {best_result['params']['lrp1']}\")\n",
    "    print(f\"  LR Phase 2: {best_result['params']['lrp2']}\")\n",
    "    print(f\"  Dropout: {best_result['params']['dropout_rate']}\")\n",
    "    print(f\"  Optimizer: {best_result['params']['optimizer']}\")\n",
    "    print(f\"  Epochs P1: {best_result['params']['epochs_p1']}\")\n",
    "    print(f\"  Epochs P2: {best_result['params']['epochs_p2']}\")\n",
    "    print(f\"  Fine-tune at: {best_result['params']['fine_tune_at']}\")\n",
    "\n",
    "# Sorted results table\n",
    "print(\"\\nüìä ALL TRIALS (sorted by accuracy):\")\n",
    "print(\"-\"*70)\n",
    "sorted_results = sorted(all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "for r in sorted_results[:10]:\n",
    "    print(f\"Trial {r['trial_id']:3d}: Acc={r['test_accuracy']:.4f}, F1={r['f1_macro']:.4f}, \"\n",
    "          f\"Arch=[{r['architecture']}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
