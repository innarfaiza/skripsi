{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71817397",
   "metadata": {},
   "source": [
    "**Ringkasan dan tujuan**\n",
    "\n",
    "Sel ini memberikan ringkasan singkat alur pipeline dan tujuan penelitian (fine‑tune ResNet untuk regresi aflatoksin). Baca sebelum menjalankan notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9397c075",
   "metadata": {},
   "source": [
    "# Finetune ResNet — Pipeline ringkas (tf.keras)\n",
    "\n",
    "Ringkasan singkat:\n",
    "- Tujuan: fine-tune backbone ResNet + head untuk regresi aflatoksin.\n",
    "- Alur yang disarankan:\n",
    "  1. Kumpulkan dan bagi data secara stratified (train/val/test).\n",
    "  2. (Opsional) Oversample hanya pada TRAIN dengan resampling filepath.\n",
    "  3. Gunakan tf.data dengan augment on-the-fly untuk TRAIN.\n",
    "  4. Phase 1: latih head dengan backbone dibekukan → simpan bobot backbone (weights-only).\n",
    "  5. Phase 2 (opsional): jalankan Keras Tuner; setiap trial memuat bobot backbone dari Phase 1 ke instansi backbone sebelum menempelkan head.\n",
    "  6. Latih akhir dengan HP terbaik → evaluasi & simpan model akhir.\n",
    "\n",
    "Catatan cepat:\n",
    "- Pastikan `base_weights_path` dihasilkan oleh fungsi `save_base_weights()` pada Phase 1.\n",
    "- Jalankan sel-sel notebook sesuai urutan: dari atas ke bawah.\n",
    "- Edit hanya sel parameter di bagian akhir sebelum menjalankan seluruh pipeline.\n",
    "\n",
    "Cara pakai singkat:\n",
    "- Buka sel parameter (akhir), atur `DATA_DIR`, `OUTPUT_DIR`, `DO_TUNING`, lalu jalankan setiap sel secara berurutan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e31563",
   "metadata": {},
   "source": [
    "### Outline fungsi (struktur per-fungsi)\n",
    "\n",
    "Berikut daftar fungsi utama yang tersedia di notebook ini beserta tujuan singkat, input, dan output yang diharapkan:\n",
    "\n",
    "- `collect_image_paths(data_dir: Path, pattern='*.png')`\n",
    "  - Tujuan: kumpulkan semua path gambar dan label (nama folder) dari `data_dir`.\n",
    "  - Input: `data_dir` (Path) — direktori dataset berstruktur folder per-kelas.\n",
    "  - Output: `(filepaths: np.ndarray, labels: np.ndarray)`\n",
    "\n",
    "- `stratified_split(filepaths, labels, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=42)`\n",
    "  - Tujuan: bagi dataset secara stratified sesuai label.\n",
    "  - Input: arrays filepaths & labels.\n",
    "  - Output: tuples `(fp_train, lb_train), (fp_val, lb_val), (fp_test, lb_test)`\n",
    "\n",
    "- `oversample_filepaths(fp_train, lb_train, target_count=None, seed=42)`\n",
    "  - Tujuan: oversample (resample filepaths) hanya untuk TRAIN agar distribusi kelas seimbang.\n",
    "  - Input: train filepaths & labels, target_count per kelas.\n",
    "  - Output: `(new_files, new_labels)` arrays\n",
    "\n",
    "- `labels_to_ppb(labels)`\n",
    "  - Tujuan: konversi label string ke nilai float (ppb) bila perlu.\n",
    "  - Output: `np.array` float32 shape `(N,)` atau `(N,1)` saat digunakan downstream.\n",
    "\n",
    "- `build_dataset_from_paths(filepaths, labels_ppb, img_size=(224,224), batch_size=32, augment=False, shuffle=False)`\n",
    "  - Tujuan: membuat `tf.data.Dataset` siap dipakai untuk training/val/test.\n",
    "  - Input: filepaths array dan labels float.\n",
    "  - Output: `tf.data.Dataset` yielding `(image, label)` batched.\n",
    "\n",
    "- `build_regression_model(input_shape=(224,224,3), backbone='resnet50', units_l1=512, dropout_rate=0.3, base_trainable=False)`\n",
    "  - Tujuan: buat model tf.keras regresi dan kembalikan `(model, base_model)`.\n",
    "  - Output: `model` (komplet) dan `base_model` (backbone) untuk disimpan/di-load terpisah.\n",
    "\n",
    "- `train_phase1(model, train_ds, val_ds, initial_epochs=30, lr=1e-6, output_dir=Path('.'))`\n",
    "  - Tujuan: latih head dengan backbone dibekukan, simpan best Phase 1.\n",
    "  - Output: `history` training\n",
    "\n",
    "- `save_base_weights(base_model, path)`\n",
    "  - Tujuan: simpan bobot backbone (weights-only) untuk digunakan tuner/final.\n",
    "  - Output: file `.h5` di `path`.\n",
    "\n",
    "- `run_keras_tuner(train_ds, val_ds, base_weights_path, max_trials=20, executions_per_trial=1, output_dir=Path('.'))`\n",
    "  - Tujuan: jalankan Keras Tuner. Untuk setiap trial, fungsi membuat backbone baru dan memuat `base_weights_path` sebelum menempelkan head.\n",
    "  - Output: `(tuner, best_hp)`\n",
    "\n",
    "- `final_train_with_best_hp(base_weights_path, best_hp, train_ds, val_ds, initial_epochs=30, fine_tune_epochs=40, output_dir=Path('.'))`\n",
    "  - Tujuan: bangun model final memakai `best_hp`, muat bobot backbone, atur `fine_tune_at`, latih, dan simpan model terbaik.\n",
    "  - Output: `(model, history)`\n",
    "\n",
    "- `evaluate_model(model, test_ds)`\n",
    "  - Tujuan: evaluasi model pada test set dan kembalikan metrics `{mae,mse,rmse,r2}`.\n",
    "\n",
    "Catatan: gunakan sel Parameter di akhir notebook untuk mengontrol eksekusi per tahapan (RUN_* flags)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470d095",
   "metadata": {},
   "source": [
    "**Imports & quick environment check**\n",
    "\n",
    "Sel ini mengimpor library yang diperlukan. Pastikan `tensorflow`, `keras-tuner` (opsional), `scikit-learn`, dan `pandas` terpasang di environment Anda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d217d863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.1 Keras Tuner: True\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports and quick environment check\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Optional: Keras Tuner (install if needed)\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "except Exception:\n",
    "    kt = None\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print('TensorFlow:', tf.__version__, 'Keras Tuner:', kt is not None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49cbaf",
   "metadata": {},
   "source": [
    "**Utilities**\n",
    "\n",
    "Fungsi utilitas: mengumpulkan path gambar, stratified split (train/val/test), oversample (train saja), dan konversi label ke nilai numerik (ppb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafcc12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Utilities: collect, stratified split, oversample (train only), label mapping\n",
    "\n",
    "def collect_image_paths(data_dir: Path, pattern='*.png'):\n",
    "    data_dir = Path(data_dir)\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for class_folder in sorted([p for p in data_dir.iterdir() if p.is_dir()]):\n",
    "        for img in class_folder.glob(pattern):\n",
    "            filepaths.append(str(img))\n",
    "            labels.append(class_folder.name)\n",
    "    return np.array(filepaths), np.array(labels)\n",
    "\n",
    "\n",
    "def stratified_split(filepaths, labels, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=42):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "    fp_train, fp_temp, lb_train, lb_temp = train_test_split(filepaths, labels, train_size=train_ratio, stratify=labels, random_state=seed)\n",
    "    val_size = val_ratio / (val_ratio + test_ratio)\n",
    "    fp_val, fp_test, lb_val, lb_test = train_test_split(fp_temp, lb_temp, train_size=val_size, stratify=lb_temp, random_state=seed)\n",
    "    return (fp_train, lb_train), (fp_val, lb_val), (fp_test, lb_test)\n",
    "\n",
    "\n",
    "def oversample_filepaths(fp_train, lb_train, target_count=None, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    classes, counts = np.unique(lb_train, return_counts=True)\n",
    "    class_to_files = {c: fp_train[lb_train == c].tolist() for c in classes}\n",
    "    if target_count is None:\n",
    "        target_count = int(max(counts))\n",
    "    new_files = []\n",
    "    new_labels = []\n",
    "    for c in classes:\n",
    "        files = class_to_files[c]\n",
    "        n = len(files)\n",
    "        if n >= target_count:\n",
    "            chosen = rng.choice(files, size=target_count, replace=False)\n",
    "        else:\n",
    "            chosen = rng.choice(files, size=target_count, replace=True)\n",
    "        new_files.extend(chosen.tolist())\n",
    "        new_labels.extend([c] * target_count)\n",
    "    combined = list(zip(new_files, new_labels))\n",
    "    rng.shuffle(combined)\n",
    "    new_files, new_labels = zip(*combined)\n",
    "    return np.array(new_files), np.array(new_labels)\n",
    "\n",
    "\n",
    "def labels_to_ppb(labels):\n",
    "    try:\n",
    "        return np.array([float(l) for l in labels], dtype=np.float32)\n",
    "    except Exception:\n",
    "        unique = sorted(list(set(labels)))\n",
    "        mapping = {c: float(i + 1) for i, c in enumerate(unique)}\n",
    "        return np.array([mapping[l] for l in labels], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb0d6f",
   "metadata": {},
   "source": [
    "**tf.data builder**\n",
    "\n",
    "Membangun pipeline `tf.data.Dataset` untuk membaca file path, decoding gambar, `preprocess_input`, augmentasi (opsional untuk train), batching, dan prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dfd0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) tf.data builder: parse, augment (train only), batch, prefetch\n",
    "\n",
    "def build_dataset_from_paths(filepaths, labels_ppb, img_size=(224,224), batch_size=32, augment=False, shuffle=False):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    paths = tf.constant(filepaths.tolist()) if isinstance(filepaths, np.ndarray) else tf.constant(list(filepaths))\n",
    "    labels = tf.constant(labels_ppb.tolist()) if isinstance(labels_ppb, np.ndarray) else tf.constant(list(labels_ppb))\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(filepaths), seed=None)\n",
    "\n",
    "    def _parse_func(path, label):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image = tf.image.resize(image, img_size)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = preprocess_input(image)\n",
    "        label = tf.cast(label, tf.float32)\n",
    "        label = tf.expand_dims(label, axis=-1)\n",
    "        return image, label\n",
    "\n",
    "    ds = ds.map(_parse_func, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            layers.RandomFlip('horizontal'),\n",
    "            layers.RandomRotation(0.05),\n",
    "            layers.RandomZoom(0.05),\n",
    "        ], name='data_augmentation')\n",
    "\n",
    "        def _augment(image, label):\n",
    "            return data_augmentation(image), label\n",
    "\n",
    "        ds = ds.map(_augment, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbe421",
   "metadata": {},
   "source": [
    "**Penjelasan sebelum sel Model builder**\n",
    "\n",
    "Sel ini mendefinisikan arsitektur model regresi: membuat backbone (ResNet50), menambahkan GlobalAveragePooling, lapisan Dense + Dropout, dan output tunggal. Fungsi mengembalikan tuple `(model, base_model)` sehingga `base_model` (backbone) dapat disimpan atau dimuat secara terpisah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36d132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Model builder: return (model, base_model) so base_model can be saved/loaded separately\n",
    "\n",
    "def build_regression_model(input_shape=(224,224,3), backbone='resnet50', units_l1=512, dropout_rate=0.3, base_trainable=False):\n",
    "    if backbone.lower() == 'resnet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError('Unsupported backbone. Use resnet50 for now.')\n",
    "    base_model.trainable = base_trainable\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(units_l1, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear', name='aflatoxin_output')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa85f3a",
   "metadata": {},
   "source": [
    "**Penjelasan sebelum sel Phase 1 (train head)**\n",
    "\n",
    "Sel ini berisi fungsi `train_phase1()` untuk melatih head dengan backbone dibekukan. Callback penting: EarlyStopping, ReduceLROnPlateau, dan ModelCheckpoint (menyimpan model terbaik Phase 1). Juga ada `save_base_weights()` untuk menyimpan bobot backbone sebagai file `.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7249904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Phase 1: train head (backbone frozen) and save backbone weights\n",
    "\n",
    "def train_phase1(model, train_ds, val_ds, initial_epochs=30, lr=1e-6, output_dir=Path('.')):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-8, verbose=1),\n",
    "        ModelCheckpoint(str(output_dir / 'best_phase1.keras'), monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['mae','mse'])\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=initial_epochs, callbacks=callbacks, verbose=1)\n",
    "    return history\n",
    "\n",
    "\n",
    "def save_base_weights(base_model, path):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    base_model.save_weights(str(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c133e",
   "metadata": {},
   "source": [
    "**Penjelasan sebelum sel Keras Tuner**\n",
    "\n",
    "Sel ini menyiapkan integrasi Keras Tuner. Fungsi `run_keras_tuner()` membuat backbone baru untuk tiap trial, memuat `base_weights_path` yang disimpan dari Phase 1, kemudian menambah head yang hyperparameternya dicari. Pastikan `DO_TUNING` hanya diaktifkan bila `keras-tuner` terpasang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04882b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Keras Tuner integration (each trial loads backbone weights into backbone)\n",
    "\n",
    "def run_keras_tuner(train_ds, val_ds, base_weights_path, max_trials=20, executions_per_trial=1, output_dir=Path('.')):\n",
    "    if kt is None:\n",
    "        raise RuntimeError('keras-tuner is not installed. Install keras-tuner to run tuning.')\n",
    "\n",
    "    def build_model_for_tuning(hp):\n",
    "        # build backbone and load weights saved from Phase1 (weights-only file)\n",
    "        base_model = ResNet50(weights=None, include_top=False, input_shape=(224,224,3))\n",
    "        try:\n",
    "            base_model.load_weights(base_weights_path)\n",
    "        except Exception as e:\n",
    "            print('Warning: could not load base weights into tuner backbone:', e)\n",
    "\n",
    "        fine_tune_at = hp.Choice('fine_tune_at', [140, 170])\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(224,224,3))\n",
    "        x = base_model(inputs, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        units_l1 = hp.Choice('units_l1', [512,128,64])\n",
    "        dropout_rate = hp.Float('dropout_rate', 0.1, 0.4, step=0.1)\n",
    "        x = layers.Dense(units_l1, activation='relu')(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        outputs = layers.Dense(1, activation='linear')(x)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        lr = hp.Choice('learning_rate', [1e-7, 1e-6, 5e-6])\n",
    "        opt_name = hp.Choice('optimizer', ['Adam','RMSprop'])\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=lr) if opt_name=='RMSprop' else tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        model.compile(optimizer=opt, loss='mse', metrics=['mae','mse'])\n",
    "        return model\n",
    "\n",
    "    tuner = kt.RandomSearch(build_model_for_tuning, objective='val_mae', max_trials=max_trials, executions_per_trial=executions_per_trial, directory=str(output_dir), project_name='resnet_finetune_tuning')\n",
    "    tuner.search(train_ds, validation_data=val_ds, epochs=10, verbose=1)\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return tuner, best_hp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1772a",
   "metadata": {},
   "source": [
    "**Penjelasan sebelum sel Final training & evaluasi**\n",
    "\n",
    "Sel ini memuat fungsi `final_train_with_best_hp()` dan `evaluate_model()` untuk: memuat bobot backbone, mengunci layer sampai `fine_tune_at`, melatih lagi sesuai HP terbaik, dan mengevaluasi model pada test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b553c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Final train with best HP and evaluation\n",
    "\n",
    "def final_train_with_best_hp(base_weights_path, best_hp, train_ds, val_ds, initial_epochs=30, fine_tune_epochs=40, output_dir=Path('.') ):\n",
    "    units = best_hp.get('units_l1')\n",
    "    dropout = best_hp.get('dropout_rate')\n",
    "    lr = best_hp.get('learning_rate')\n",
    "    opt_name = best_hp.get('optimizer')\n",
    "    fine_tune_at = best_hp.get('fine_tune_at')\n",
    "\n",
    "    model, base_model = build_regression_model(units_l1=units, dropout_rate=dropout, base_trainable=True)\n",
    "    try:\n",
    "        base_model.load_weights(base_weights_path)\n",
    "    except Exception as e:\n",
    "        print('Warning: could not load base weights into final model:', e)\n",
    "\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=lr) if opt_name=='RMSprop' else tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae','mse'])\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-8, verbose=1),\n",
    "        ModelCheckpoint(str(output_dir / 'best_final.keras'), monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "    total_epochs = initial_epochs + fine_tune_epochs\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=total_epochs, initial_epoch=0, callbacks=callbacks, verbose=1)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_ds):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for x,y in test_ds:\n",
    "        preds = model.predict(x, verbose=0).flatten()\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(y.numpy().flatten().tolist())\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'mae': float(mae), 'mse': float(mse), 'rmse': float(rmse), 'r2': float(r2)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88291a10",
   "metadata": {},
   "source": [
    "**Penjelasan sebelum sel Parameter & Run example**\n",
    "\n",
    "Sel parameter di bawah berisi variabel yang harus Anda sesuaikan sebelum menjalankan pipeline (lokasi data, ukuran gambar, batch size, opsi tuning, seed, dsb.). Setelah disesuaikan, jalankan sel-sel dari atas ke bawah."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8bf2d",
   "metadata": {},
   "source": [
    "## Outline & menjalankan per tahapan\n",
    "\n",
    "Notebook sudah diatur per bagian (Imports, Utilities, tf.data builder, Model builder, Phase 1, Keras Tuner, Final training, Parameter). Gunakan sel Parameter (akhir) untuk mengontrol eksekusi per-tahapan melalui flag RUN_*. Jika ingin menjalankan manual, jalankan sel yang diperlukan secara langsung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4c9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 871\n",
      "Counts - train: 609 val: 174 test: 88\n",
      "Epoch 1/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758ms/step - loss: 6.8126 - mae: 2.3227 - mse: 6.8126\n",
      "Epoch 1: val_loss improved from inf to 4.91202, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 860ms/step - loss: 6.8093 - mae: 2.3220 - mse: 6.8093 - val_loss: 4.9120 - val_mae: 2.0233 - val_mse: 4.9120 - learning_rate: 1.0000e-06\n",
      "Epoch 2/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788ms/step - loss: 6.2010 - mae: 2.1939 - mse: 6.2010\n",
      "Epoch 2: val_loss improved from 4.91202 to 4.18830, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 872ms/step - loss: 6.1967 - mae: 2.1930 - mse: 6.1967 - val_loss: 4.1883 - val_mae: 1.8385 - val_mse: 4.1883 - learning_rate: 1.0000e-06\n",
      "Epoch 3/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762ms/step - loss: 5.5528 - mae: 2.0306 - mse: 5.5528\n",
      "Epoch 3: val_loss improved from 4.18830 to 3.55785, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 846ms/step - loss: 5.5510 - mae: 2.0303 - mse: 5.5510 - val_loss: 3.5578 - val_mae: 1.6615 - val_mse: 3.5578 - learning_rate: 1.0000e-06\n",
      "Epoch 4/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773ms/step - loss: 4.9033 - mae: 1.8939 - mse: 4.9033\n",
      "Epoch 4: val_loss improved from 3.55785 to 3.02270, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 856ms/step - loss: 4.9024 - mae: 1.8936 - mse: 4.9024 - val_loss: 3.0227 - val_mae: 1.4947 - val_mse: 3.0227 - learning_rate: 1.0000e-06\n",
      "Epoch 5/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773ms/step - loss: 4.4955 - mae: 1.8026 - mse: 4.4955\n",
      "Epoch 5: val_loss improved from 3.02270 to 2.56339, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 858ms/step - loss: 4.4929 - mae: 1.8019 - mse: 4.4929 - val_loss: 2.5634 - val_mae: 1.3381 - val_mse: 2.5634 - learning_rate: 1.0000e-06\n",
      "Epoch 6/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777ms/step - loss: 3.9794 - mae: 1.6603 - mse: 3.9794\n",
      "Epoch 6: val_loss improved from 2.56339 to 2.17229, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 861ms/step - loss: 3.9773 - mae: 1.6598 - mse: 3.9773 - val_loss: 2.1723 - val_mae: 1.2030 - val_mse: 2.1723 - learning_rate: 1.0000e-06\n",
      "Epoch 7/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771ms/step - loss: 3.7931 - mae: 1.6178 - mse: 3.7931\n",
      "Epoch 7: val_loss improved from 2.17229 to 1.82997, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 855ms/step - loss: 3.7887 - mae: 1.6167 - mse: 3.7887 - val_loss: 1.8300 - val_mae: 1.0918 - val_mse: 1.8300 - learning_rate: 1.0000e-06\n",
      "Epoch 8/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772ms/step - loss: 3.0824 - mae: 1.4416 - mse: 3.0824\n",
      "Epoch 8: val_loss improved from 1.82997 to 1.55119, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 856ms/step - loss: 3.0840 - mae: 1.4420 - mse: 3.0840 - val_loss: 1.5512 - val_mae: 1.0034 - val_mse: 1.5512 - learning_rate: 1.0000e-06\n",
      "Epoch 9/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781ms/step - loss: 2.8446 - mae: 1.3776 - mse: 2.8446\n",
      "Epoch 9: val_loss improved from 1.55119 to 1.31438, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 865ms/step - loss: 2.8448 - mae: 1.3778 - mse: 2.8448 - val_loss: 1.3144 - val_mae: 0.9181 - val_mse: 1.3144 - learning_rate: 1.0000e-06\n",
      "Epoch 10/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771ms/step - loss: 2.7069 - mae: 1.3476 - mse: 2.7069\n",
      "Epoch 10: val_loss improved from 1.31438 to 1.12398, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 856ms/step - loss: 2.7045 - mae: 1.3470 - mse: 2.7045 - val_loss: 1.1240 - val_mae: 0.8396 - val_mse: 1.1240 - learning_rate: 1.0000e-06\n",
      "Epoch 11/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782ms/step - loss: 2.4307 - mae: 1.2839 - mse: 2.4307\n",
      "Epoch 11: val_loss improved from 1.12398 to 0.97489, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 866ms/step - loss: 2.4293 - mae: 1.2834 - mse: 2.4293 - val_loss: 0.9749 - val_mae: 0.7747 - val_mse: 0.9749 - learning_rate: 1.0000e-06\n",
      "Epoch 12/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784ms/step - loss: 2.2289 - mae: 1.2311 - mse: 2.2289\n",
      "Epoch 12: val_loss improved from 0.97489 to 0.86887, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 873ms/step - loss: 2.2269 - mae: 1.2304 - mse: 2.2269 - val_loss: 0.8689 - val_mae: 0.7219 - val_mse: 0.8689 - learning_rate: 1.0000e-06\n",
      "Epoch 13/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817ms/step - loss: 1.9986 - mae: 1.1604 - mse: 1.9986\n",
      "Epoch 13: val_loss improved from 0.86887 to 0.79983, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 904ms/step - loss: 1.9978 - mae: 1.1602 - mse: 1.9978 - val_loss: 0.7998 - val_mae: 0.7019 - val_mse: 0.7998 - learning_rate: 1.0000e-06\n",
      "Epoch 14/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788ms/step - loss: 1.9845 - mae: 1.1733 - mse: 1.9845\n",
      "Epoch 14: val_loss improved from 0.79983 to 0.75995, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 873ms/step - loss: 1.9829 - mae: 1.1727 - mse: 1.9829 - val_loss: 0.7600 - val_mae: 0.7022 - val_mse: 0.7600 - learning_rate: 1.0000e-06\n",
      "Epoch 15/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - loss: 1.7374 - mae: 1.0815 - mse: 1.7374\n",
      "Epoch 15: val_loss improved from 0.75995 to 0.74617, saving model to outputs_compact\\best_phase1.keras\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 897ms/step - loss: 1.7371 - mae: 1.0816 - mse: 1.7371 - val_loss: 0.7462 - val_mae: 0.7131 - val_mse: 0.7462 - learning_rate: 1.0000e-06\n",
      "Epoch 16/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817ms/step - loss: 1.5517 - mae: 1.0400 - mse: 1.5517\n",
      "Epoch 16: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 895ms/step - loss: 1.5527 - mae: 1.0405 - mse: 1.5527 - val_loss: 0.7516 - val_mae: 0.7295 - val_mse: 0.7516 - learning_rate: 1.0000e-06\n",
      "Epoch 17/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814ms/step - loss: 1.6048 - mae: 1.0712 - mse: 1.6048\n",
      "Epoch 17: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 893ms/step - loss: 1.6035 - mae: 1.0706 - mse: 1.6035 - val_loss: 0.7711 - val_mae: 0.7485 - val_mse: 0.7711 - learning_rate: 1.0000e-06\n",
      "Epoch 18/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786ms/step - loss: 1.5234 - mae: 1.0279 - mse: 1.5234\n",
      "Epoch 18: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 861ms/step - loss: 1.5229 - mae: 1.0278 - mse: 1.5229 - val_loss: 0.7999 - val_mae: 0.7669 - val_mse: 0.7999 - learning_rate: 1.0000e-06\n",
      "Epoch 19/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775ms/step - loss: 1.3898 - mae: 0.9910 - mse: 1.3898\n",
      "Epoch 19: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 849ms/step - loss: 1.3904 - mae: 0.9914 - mse: 1.3904 - val_loss: 0.8356 - val_mae: 0.7847 - val_mse: 0.8356 - learning_rate: 1.0000e-06\n",
      "Epoch 20/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771ms/step - loss: 1.4009 - mae: 1.0138 - mse: 1.4009\n",
      "Epoch 20: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 844ms/step - loss: 1.4006 - mae: 1.0136 - mse: 1.4006 - val_loss: 0.8754 - val_mae: 0.8015 - val_mse: 0.8754 - learning_rate: 1.0000e-06\n",
      "Epoch 21/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775ms/step - loss: 1.4070 - mae: 1.0122 - mse: 1.4070\n",
      "Epoch 21: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 850ms/step - loss: 1.4067 - mae: 1.0122 - mse: 1.4067 - val_loss: 0.9099 - val_mae: 0.8144 - val_mse: 0.9099 - learning_rate: 1.0000e-06\n",
      "Epoch 22/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774ms/step - loss: 1.2725 - mae: 0.9575 - mse: 1.2725\n",
      "Epoch 22: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 848ms/step - loss: 1.2736 - mae: 0.9580 - mse: 1.2736 - val_loss: 0.9438 - val_mae: 0.8259 - val_mse: 0.9438 - learning_rate: 1.0000e-06\n",
      "Epoch 23/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766ms/step - loss: 1.3058 - mae: 0.9735 - mse: 1.3058\n",
      "Epoch 23: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 840ms/step - loss: 1.3059 - mae: 0.9736 - mse: 1.3059 - val_loss: 0.9741 - val_mae: 0.8353 - val_mse: 0.9741 - learning_rate: 1.0000e-06\n",
      "Epoch 24/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783ms/step - loss: 1.2741 - mae: 0.9664 - mse: 1.2741\n",
      "Epoch 24: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 858ms/step - loss: 1.2746 - mae: 0.9666 - mse: 1.2746 - val_loss: 1.0055 - val_mae: 0.8464 - val_mse: 1.0055 - learning_rate: 1.0000e-06\n",
      "Epoch 25/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784ms/step - loss: 1.2949 - mae: 0.9675 - mse: 1.2949\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-07.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 857ms/step - loss: 1.2947 - mae: 0.9674 - mse: 1.2947 - val_loss: 1.0296 - val_mae: 0.8548 - val_mse: 1.0296 - learning_rate: 1.0000e-06\n",
      "Epoch 26/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773ms/step - loss: 1.2964 - mae: 0.9686 - mse: 1.2964\n",
      "Epoch 26: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 847ms/step - loss: 1.2963 - mae: 0.9685 - mse: 1.2963 - val_loss: 1.0434 - val_mae: 0.8594 - val_mse: 1.0434 - learning_rate: 5.0000e-07\n",
      "Epoch 27/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785ms/step - loss: 1.3118 - mae: 0.9873 - mse: 1.3118\n",
      "Epoch 27: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 858ms/step - loss: 1.3114 - mae: 0.9871 - mse: 1.3114 - val_loss: 1.0543 - val_mae: 0.8629 - val_mse: 1.0543 - learning_rate: 5.0000e-07\n",
      "Epoch 28/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769ms/step - loss: 1.3025 - mae: 0.9724 - mse: 1.3025\n",
      "Epoch 28: val_loss did not improve from 0.74617\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 843ms/step - loss: 1.3022 - mae: 0.9723 - mse: 1.3022 - val_loss: 1.0662 - val_mae: 0.8668 - val_mse: 1.0662 - learning_rate: 5.0000e-07\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filename must end in `.weights.h5`. Received: filepath=outputs_compact\\base_weights.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RUN_SAVE_BASE:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[1;32m---> 77\u001b[0m         \u001b[43msave_base_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_weights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved backbone weights to\u001b[39m\u001b[38;5;124m'\u001b[39m, base_weights_path)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36msave_base_weights\u001b[1;34m(base_model, path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_base_weights\u001b[39m(base_model, path):\n\u001b[0;32m     15\u001b[0m     Path(path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:211\u001b[0m, in \u001b[0;36msave_weights\u001b[1;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.saving.save_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_weights\u001b[39m(model, filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    212\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filename must end in `.weights.h5`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m         exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filepath)\n",
      "\u001b[1;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=outputs_compact\\base_weights.h5"
     ]
    }
   ],
   "source": [
    "# 8) Parameters & run example (edit then run in order)\n",
    "\n",
    "# Stage control flags: set True untuk mengeksekusi tahapan ini saat menjalankan SEL PARAMETER\n",
    "RUN_COLLECT = True        # collect & stratified split\n",
    "RUN_OVERSAMPLE = True     # oversample train only (resample filepaths)\n",
    "RUN_DATASET = True        # build tf.data datasets\n",
    "RUN_PHASE1 = True         # train phase1 (head, backbone frozen)\n",
    "RUN_SAVE_BASE = True      # save base_model weights after Phase1\n",
    "RUN_TUNER = False         # jalankan Keras Tuner\n",
    "RUN_FINAL = True          # final training & evaluate\n",
    "\n",
    "# Parameters (edit as needed)\n",
    "DATA_DIR = Path('data valid merah')\n",
    "OUTPUT_DIR = Path('outputs_compact')\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCHS = 30\n",
    "PHASE1_LR = 1e-6\n",
    "FINE_TUNE_EPOCHS = 40\n",
    "OVERSAMPLE = True\n",
    "TARGET_COUNT = 450\n",
    "SEED = 42\n",
    "DO_TUNING = RUN_TUNER  # internal convenience\n",
    "MAX_TRIALS = 20\n",
    "EXECUTIONS_PER_TRIAL = 1\n",
    "\n",
    "# reproducibility seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# 1) collect and split\n",
    "if RUN_COLLECT:\n",
    "    filepaths, labels = collect_image_paths(DATA_DIR)\n",
    "    print('Total images found:', len(filepaths))\n",
    "    (fp_train, lb_train), (fp_val, lb_val), (fp_test, lb_test) = stratified_split(\n",
    "        filepaths, labels, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=SEED\n",
    "    )\n",
    "    print('Counts - train:', len(fp_train), 'val:', len(fp_val), 'test:', len(fp_test))\n",
    "else:\n",
    "    required = ['fp_train','lb_train','fp_val','lb_val','fp_test','lb_test']\n",
    "    if not all(name in globals() for name in required):\n",
    "        raise RuntimeError('Skipping collect but split variables are missing. Set RUN_COLLECT=True or define split variables manually.')\n",
    "\n",
    "# 2) oversample train only (resample filepaths)\n",
    "if RUN_OVERSAMPLE:\n",
    "    fp_train_os, lb_train_os = oversample_filepaths(fp_train, lb_train, target_count=TARGET_COUNT, seed=SEED)\n",
    "else:\n",
    "    fp_train_os, lb_train_os = fp_train, lb_train\n",
    "\n",
    "# 3) datasets\n",
    "if RUN_DATASET:\n",
    "    y_train = labels_to_ppb(lb_train_os)\n",
    "    y_val = labels_to_ppb(lb_val)\n",
    "    y_test = labels_to_ppb(lb_test)\n",
    "\n",
    "    train_ds = build_dataset_from_paths(fp_train_os, y_train, img_size=(IMG_SIZE,IMG_SIZE), batch_size=BATCH_SIZE, augment=True, shuffle=True)\n",
    "    val_ds = build_dataset_from_paths(fp_val, y_val, img_size=(IMG_SIZE,IMG_SIZE), batch_size=BATCH_SIZE, augment=False, shuffle=False)\n",
    "    test_ds = build_dataset_from_paths(fp_test, y_test, img_size=(IMG_SIZE,IMG_SIZE), batch_size=BATCH_SIZE, augment=False, shuffle=False)\n",
    "else:\n",
    "    if 'train_ds' not in globals():\n",
    "        raise RuntimeError('Skipping dataset build but dataset variables not found. Set RUN_DATASET=True or define train_ds/val_ds/test_ds manually.')\n",
    "\n",
    "# 4) phase1 train\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if RUN_PHASE1:\n",
    "    model, base_model = build_regression_model(input_shape=(IMG_SIZE,IMG_SIZE,3), backbone='resnet50', units_l1=512, dropout_rate=0.3, base_trainable=False)\n",
    "    history_p1 = train_phase1(model, train_ds, val_ds, initial_epochs=INITIAL_EPOCHS, lr=PHASE1_LR, output_dir=OUTPUT_DIR)\n",
    "else:\n",
    "    if 'base_model' not in globals():\n",
    "        print('Phase1 skipped and `base_model` not found in namespace. If you plan to run tuner/final, ensure `base_weights_path` exists or set RUN_PHASE1=True to produce it.')\n",
    "\n",
    "# 5) save backbone weights for tuner/final\n",
    "base_weights_path = OUTPUT_DIR / 'base_weights.h5'\n",
    "if RUN_SAVE_BASE:\n",
    "    if 'base_model' in globals():\n",
    "        save_base_weights(base_model, base_weights_path)\n",
    "        print('Saved backbone weights to', base_weights_path)\n",
    "    else:\n",
    "        print('Cannot save base weights: `base_model` not defined. Run Phase1 first or set base_weights_path manually.')\n",
    "\n",
    "# 6) tuner (optional)\n",
    "if RUN_TUNER:\n",
    "    if not DO_TUNING:\n",
    "        raise RuntimeError('RUN_TUNER=True but keras-tuner not available or DO_TUNING flag mismatch.')\n",
    "    tuner, best_hp = run_keras_tuner(train_ds, val_ds, str(base_weights_path), max_trials=MAX_TRIALS, executions_per_trial=EXECUTIONS_PER_TRIAL, output_dir=OUTPUT_DIR)\n",
    "else:\n",
    "    class DummyHP(dict):\n",
    "        def get(self, k, default=None):\n",
    "            return {'units_l1':512, 'dropout_rate':0.3, 'learning_rate':5e-6, 'optimizer':'Adam', 'fine_tune_at':140}.get(k, default)\n",
    "    best_hp = DummyHP()\n",
    "\n",
    "# 7) final train and evaluate\n",
    "if RUN_FINAL:\n",
    "    final_model, history_final = final_train_with_best_hp(str(base_weights_path), best_hp, train_ds, val_ds, initial_epochs=INITIAL_EPOCHS, fine_tune_epochs=FINE_TUNE_EPOCHS, output_dir=OUTPUT_DIR)\n",
    "    metrics = evaluate_model(final_model, test_ds)\n",
    "    print('Test metrics:', metrics)\n",
    "\n",
    "    # 8) save final model\n",
    "    final_model.save(OUTPUT_DIR / 'aflatoxin_final_model.keras')\n",
    "    print('Saved final model and artifacts in', OUTPUT_DIR)\n",
    "else:\n",
    "    print('Final training skipped.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
