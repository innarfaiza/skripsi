{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ffb277",
   "metadata": {},
   "source": [
    "## üìö PENJELASAN KONFIGURASI - RESNET18\n",
    "\n",
    "### ‚ö†Ô∏è CATATAN PENTING\n",
    "**ResNet18 TIDAK tersedia di `tensorflow.keras.applications`!**\n",
    "\n",
    "Notebook ini menggunakan **Custom ResNet18** yang dibangun manual dengan Keras layers.\n",
    "Model ini **TIDAK memiliki pre-trained weights** dari ImageNet.\n",
    "\n",
    "| Model | Parameter | Pre-trained? | Layers |\n",
    "|-------|-----------|--------------|--------|\n",
    "| ResNet18 (Custom) | ~11.2 juta | ‚ùå Tidak | ~60 |\n",
    "| ResNet50 (Keras) | ~25.6 juta | ‚úÖ Ya | ~175 |\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Label Smoothing\n",
    "Label smoothing mengubah target dari \"keras\" (hard) menjadi \"lembut\" (soft):\n",
    "\n",
    "```\n",
    "TANPA Label Smoothing (Œ± = 0):\n",
    "Target Kelas 2: [0, 1, 0, 0]  ‚Üê Model DIPAKSA 100% yakin\n",
    "\n",
    "DENGAN Label Smoothing (Œ± = 0.1):\n",
    "Target Kelas 2: [0.025, 0.925, 0.025, 0.025]  ‚Üê Model BOLEH sedikit ragu\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Class Weights Moderat (Sqrt Balanced)\n",
    "Memberikan bobot berbeda per kelas dalam perhitungan loss:\n",
    "\n",
    "| Kelas | Data Unik | Weight | Efek |\n",
    "|-------|-----------|--------|------|\n",
    "| 1 | 128 | 1.18 | Sedikit boost |\n",
    "| 2 | 256 | 0.83 | Dikurangi (mayoritas) |\n",
    "| 3 | 161 | 1.05 | Hampir netral |\n",
    "| 4 | 63 | 1.67 | Di-boost (minoritas) |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Perbedaan Parameter dengan ResNet50\n",
    "\n",
    "| Parameter | ResNet50 | ResNet18 | Alasan |\n",
    "|-----------|----------|----------|--------|\n",
    "| FINE_TUNE_AT | 140 | 40 | ResNet18 hanya ~60 layers |\n",
    "| LR_PHASE_1 | 1e-4 | 1e-3 | Training from scratch butuh LR lebih tinggi |\n",
    "| LR_PHASE_2 | 5e-5 | 1e-4 | Fine-tuning lebih agresif |\n",
    "| EPOCHS | Lebih sedikit | Lebih banyak | Tanpa pre-trained butuh lebih lama |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 1. SETUP & LOAD DATA (MENGGUNAKAN PREPROCESS YANG SUDAH ADA)\n",
    "# =================================================================================================\n",
    "\n",
    "# Memanggil file preprocess_resnet18.ipynb agar variabel train_ds, val_ds, test_ds tersedia\n",
    "# Pastikan file preprocess_resnet18.ipynb berada di folder yang sama\n",
    "%run preprocess_resnet18.ipynb\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, GlobalAveragePooling2D, Dropout, Input,\n",
    "    Conv2D, BatchNormalization, Activation, MaxPooling2D, Add\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import AdamW, SGD, RMSprop, Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, name='focal_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        pt = y_true * y_pred\n",
    "        pt = tf.reduce_sum(pt, axis=-1, keepdims=True)\n",
    "        focal_weight = tf.pow(1.0 - pt, self.gamma)\n",
    "        \n",
    "        # Apply focal loss\n",
    "        focal_loss = self.alpha * focal_weight * tf.reduce_sum(ce, axis=-1, keepdims=True)\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resnet18-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 1.5 CUSTOM RESNET18 BUILDER\n",
    "# =================================================================================================\n",
    "# ResNet18 tidak tersedia di keras.applications, jadi kita bangun manual\n",
    "\n",
    "def basic_block(x, filters, stride=1, downsample=None, name=None):\n",
    "    \"\"\"\n",
    "    Basic Block untuk ResNet18/34\n",
    "    Struktur: x ‚Üí Conv3x3 ‚Üí BN ‚Üí ReLU ‚Üí Conv3x3 ‚Üí BN ‚Üí Add(x) ‚Üí ReLU\n",
    "    \"\"\"\n",
    "    identity = x\n",
    "    \n",
    "    # First conv\n",
    "    out = Conv2D(filters, 3, strides=stride, padding='same', \n",
    "                 use_bias=False, name=f'{name}_conv1')(x)\n",
    "    out = BatchNormalization(name=f'{name}_bn1')(out)\n",
    "    out = Activation('relu', name=f'{name}_relu1')(out)\n",
    "    \n",
    "    # Second conv\n",
    "    out = Conv2D(filters, 3, strides=1, padding='same', \n",
    "                 use_bias=False, name=f'{name}_conv2')(out)\n",
    "    out = BatchNormalization(name=f'{name}_bn2')(out)\n",
    "    \n",
    "    # Shortcut connection\n",
    "    if downsample is not None:\n",
    "        identity = downsample(x)\n",
    "    \n",
    "    out = Add(name=f'{name}_add')([out, identity])\n",
    "    out = Activation('relu', name=f'{name}_relu2')(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def make_layer(x, filters, blocks, stride=1, name=None):\n",
    "    \"\"\"\n",
    "    Membuat layer yang terdiri dari beberapa basic blocks\n",
    "    \"\"\"\n",
    "    downsample = None\n",
    "    \n",
    "    # Jika stride != 1 atau jumlah filter berubah, perlu downsample\n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        downsample = Sequential([\n",
    "            Conv2D(filters, 1, strides=stride, use_bias=False),\n",
    "            BatchNormalization()\n",
    "        ], name=f'{name}_downsample')\n",
    "    \n",
    "    # First block (mungkin perlu downsample)\n",
    "    x = basic_block(x, filters, stride, downsample, name=f'{name}_block1')\n",
    "    \n",
    "    # Remaining blocks\n",
    "    for i in range(1, blocks):\n",
    "        x = basic_block(x, filters, name=f'{name}_block{i+1}')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_resnet18_base(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Membangun base ResNet18 (tanpa classification head)\n",
    "    \n",
    "    Struktur:\n",
    "    - Conv1: 7x7, 64 filters, stride 2\n",
    "    - MaxPool: 3x3, stride 2  \n",
    "    - Layer1: 2 basic blocks, 64 filters\n",
    "    - Layer2: 2 basic blocks, 128 filters, stride 2\n",
    "    - Layer3: 2 basic blocks, 256 filters, stride 2\n",
    "    - Layer4: 2 basic blocks, 512 filters, stride 2\n",
    "    - Global Average Pooling ‚Üí Output: 512-dim vector\n",
    "    \n",
    "    Total ~60 layers\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Initial convolution (conv1)\n",
    "    x = Conv2D(64, 7, strides=2, padding='same', use_bias=False, name='conv1')(inputs)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    x = MaxPooling2D(3, strides=2, padding='same', name='maxpool')(x)\n",
    "    \n",
    "    # Residual layers\n",
    "    x = make_layer(x, 64, 2, stride=1, name='layer1')   # conv2_x: 2 blocks\n",
    "    x = make_layer(x, 128, 2, stride=2, name='layer2')  # conv3_x: 2 blocks\n",
    "    x = make_layer(x, 256, 2, stride=2, name='layer3')  # conv4_x: 2 blocks\n",
    "    x = make_layer(x, 512, 2, stride=2, name='layer4')  # conv5_x: 2 blocks\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = GlobalAveragePooling2D(name='avgpool')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x, name='resnet18_base')\n",
    "    return model\n",
    "\n",
    "# Test build dan tampilkan info\n",
    "print(\"=\"*60)\n",
    "print(\"CUSTOM RESNET18 ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStruktur ResNet18:\")\n",
    "print(\"  - Conv1: 7x7, 64 filters, stride 2\")\n",
    "print(\"  - MaxPool: 3x3, stride 2\")\n",
    "print(\"  - Layer1: 2 basic blocks √ó 64 filters\")\n",
    "print(\"  - Layer2: 2 basic blocks √ó 128 filters\")\n",
    "print(\"  - Layer3: 2 basic blocks √ó 256 filters\")\n",
    "print(\"  - Layer4: 2 basic blocks √ó 512 filters\")\n",
    "print(\"  - Global Average Pooling ‚Üí 512-dim\")\n",
    "\n",
    "_test_base = build_resnet18_base()\n",
    "print(f\"\\n‚úì Total layers in base model: {len(_test_base.layers)}\")\n",
    "print(f\"‚úì Base model parameters: {_test_base.count_params():,}\")\n",
    "del _test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 2. KONFIGURASI EKSPERIMEN (UBAH PARAMETER DI SINI SETIAP KALI RUNNING)\n",
    "# =================================================================================================\n",
    "# Ini adalah \"Control Panel\" kamu. Ubah angka di sini, lalu Run All.\n",
    "\n",
    "# --- Parameter Arsitektur Model ---\n",
    "LAYER_CONFIG = [256, 128]   # Konfigurasi layer Dense bertingkat\n",
    "                            # Lebih sederhana dari ResNet50 karena model lebih kecil\n",
    "DROPOUT_RATE = 0.5          # 0.5 artinya 50% neuron dimatikan acak saat training\n",
    "FINE_TUNE_AT = 40           # Layer ResNet18 (total ~60) mulai dari mana kita 'cairkan'\n",
    "                            # ResNet18 lebih kecil, jadi fine-tune dari layer lebih awal\n",
    "\n",
    "# --- Parameter Training Phase 1 (Feature Extraction - Base Model Beku) ---\n",
    "LR_PHASE_1 = 1e-3           # Learning Rate lebih tinggi karena training from scratch\n",
    "EPOCHS_PHASE_1 = 40         # Epoch lebih banyak karena tanpa pre-trained weights\n",
    "\n",
    "# --- Parameter Training Phase 2 (Fine Tuning - Base Model Cair Sebagian) ---\n",
    "LR_PHASE_2 = 1e-4           # LR untuk fine-tuning\n",
    "EPOCHS_PHASE_2 = 30         # Epoch lanjutan\n",
    "\n",
    "# --- Lainnya ---\n",
    "BATCH_SIZE = 32             # Jumlah gambar yang diproses sekali jalan\n",
    "OPTIMIZER_NAME = 'Adam'     # Pilihan: 'Adam', 'AdamW', 'SGD', 'RMSprop'\n",
    "LOG_FILE_PATH = 'experiment_log_resnet18.csv'  # Nama file untuk menyimpan hasil\n",
    "MODEL_SAVE_PATH = 'best_resnet18_aflatoxin.keras'\n",
    "\n",
    "# --- Regularisasi ---\n",
    "LABEL_SMOOTHING = 0         # Label smoothing (0 = off)\n",
    "\n",
    "# --- Class Weights Moderat (Sqrt Balanced berdasarkan foto UNIK) ---\n",
    "USE_CLASS_WEIGHTS = True    # Set False jika ingin menonaktifkan\n",
    "CLASS_WEIGHTS = {\n",
    "    0: 1.18,    # Kelas 1 (128 foto unik)\n",
    "    1: 0.83,    # Kelas 2 (256 foto unik)\n",
    "    2: 1.05,    # Kelas 3 (161 foto unik)\n",
    "    3: 1.67,    # Kelas 4 (63 foto unik)\n",
    "}\n",
    "\n",
    "# --- Print Konfigurasi ---\n",
    "print(\"=\"*60)\n",
    "print(\"KONFIGURASI EKSPERIMEN RESNET18\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìê Arsitektur:\")\n",
    "print(f\"   Layer Config: {LAYER_CONFIG}\")\n",
    "print(f\"   Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"   Fine-tune at: {FINE_TUNE_AT}\")\n",
    "print(f\"\\nüìà Phase 1:\")\n",
    "print(f\"   LR: {LR_PHASE_1}, Epochs: {EPOCHS_PHASE_1}\")\n",
    "print(f\"\\nüìà Phase 2:\")\n",
    "print(f\"   LR: {LR_PHASE_2}, Epochs: {EPOCHS_PHASE_2}\")\n",
    "print(f\"\\n‚öñÔ∏è Class Weights: {USE_CLASS_WEIGHTS}\")\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    print(f\"   {CLASS_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf83390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 3. MEMBANGUN MODEL (ARSITEKTUR RESNET18)\n",
    "# =================================================================================================\n",
    "\n",
    "def build_model_experiment():\n",
    "    # Menggunakan Custom ResNet18 sebagai base\n",
    "    # CATATAN: Tidak ada pre-trained weights!\n",
    "    base_model = build_resnet18_base(input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze Base Model: Kita kunci bobot ResNet agar tidak berubah di Fase 1\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    \n",
    "    for units in LAYER_CONFIG:\n",
    "        # Tambah Dense Layer\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "        if DROPOUT_RATE > 0:\n",
    "            x = Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # Output Layer (4 Kelas)\n",
    "    output = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model, base_model\n",
    "\n",
    "# Inisialisasi Model\n",
    "model, base_model = build_model_experiment()\n",
    "\n",
    "# Setup Optimizer Dinamis sesuai Konfigurasi\n",
    "if OPTIMIZER_NAME.lower() == 'adam':\n",
    "    opt = Adam(learning_rate=LR_PHASE_1)\n",
    "elif OPTIMIZER_NAME.lower() == 'sgd':\n",
    "    opt = SGD(learning_rate=LR_PHASE_1, momentum=0.9)\n",
    "elif OPTIMIZER_NAME.lower() == 'adamw':\n",
    "    opt = AdamW(learning_rate=LR_PHASE_1)\n",
    "else:\n",
    "    opt = RMSprop(learning_rate=LR_PHASE_1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt, \n",
    "    loss=FocalLoss(alpha=1.0, gamma=2.0), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] Model dibangun dengan Optimizer: {OPTIMIZER_NAME} | LR Awal: {LR_PHASE_1}\")\n",
    "print(f\"‚úì Optimizer: {OPTIMIZER_NAME} | LR Awal: {LR_PHASE_1}\")\n",
    "print(f\"‚úì Loss: Focal Loss (alpha=1.0, gamma=2.0)\")\n",
    "print(f\"‚úì Metrics: accuracy\")\n",
    "print(f\"Model compiled successfully!\")\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
    "print(f\"Base model layers: {len(base_model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcefbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up training callbacks...\")\n",
    "\n",
    "# Training callbacks for better training control\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,        # Lebih sabar karena training from scratch\n",
    "        mode='max',\n",
    "        restore_best_weights=True, \n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úì EarlyStopping (monitor='val_accuracy', patience=15)\")\n",
    "print(\"‚úì ReduceLROnPlateau (factor=0.5, patience=8)\")\n",
    "print(f\"‚úì ModelCheckpoint (save to: {MODEL_SAVE_PATH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98032e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 4. TRAINING PHASE 1 (WARM UP)\n",
    "# =================================================================================================\n",
    "# Tujuannya agar layer Dense baru beradaptasi dengan fitur dari ResNet sebelum ResNet-nya kita utak-atik.\n",
    "\n",
    "active_class_weights = CLASS_WEIGHTS if USE_CLASS_WEIGHTS else None\n",
    "\n",
    "print(f\"\\n[INFO] === PHASE 1: Training Head Only ({EPOCHS_PHASE_1} Epochs) ===\")\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    print(f\"‚úì Class Weights aktif: {CLASS_WEIGHTS}\")\n",
    "\n",
    "history_1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_PHASE_1,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=active_class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "p1_best_val_acc = max(history_1.history['val_accuracy'])\n",
    "print(f\"\\n‚úì Phase 1 Selesai!\")\n",
    "print(f\"‚úì Best Validation Accuracy Phase 1: {p1_best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 5. TRAINING PHASE 2 (FINE TUNING)\n",
    "# =================================================================================================\n",
    "# Sekarang kita cairkan sebagian layer ResNet18 untuk menyesuaikan fitur spesifik\n",
    "\n",
    "# --- LANGKAH PENTING: MUAT BOBOT TERBAIK DARI FASE 1 ---\n",
    "try:\n",
    "    model.load_weights(MODEL_SAVE_PATH)\n",
    "    print(\"‚úÖ Bobot terbaik dari Phase 1 berhasil dimuat.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Peringatan: Gagal memuat bobot dari {MODEL_SAVE_PATH}. Error: {e}\")\n",
    "    print(\"   Melanjutkan dengan bobot terakhir Phase 1.\")\n",
    "    \n",
    "print(f\"\\n[INFO] === PHASE 2: Fine Tuning (Start Layer {FINE_TUNE_AT}, {EPOCHS_PHASE_2} Epochs) ===\")\n",
    "\n",
    "total_epochs = EPOCHS_PHASE_1 + EPOCHS_PHASE_2\n",
    "base_model.trainable = True  # Unfreeze base model\n",
    "\n",
    "# Freeze ulang layer-layer awal (Low-level features seperti garis/sudut biasanya sudah bagus)\n",
    "# Kita hanya ingin melatih layer-layer akhir (High-level features)\n",
    "for layer in base_model.layers[:FINE_TUNE_AT]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Hitung jumlah layer yang di-unfreeze\n",
    "trainable_count = sum([1 for layer in base_model.layers if layer.trainable])\n",
    "print(f\"‚úì {trainable_count} layers di-unfreeze untuk fine-tuning\")\n",
    "\n",
    "# Compile ulang wajib dilakukan setelah mengubah trainable status\n",
    "# Gunakan LR yang lebih kecil untuk fine-tuning\n",
    "if OPTIMIZER_NAME.lower() == 'adam':\n",
    "    opt_ft = Adam(learning_rate=LR_PHASE_2)\n",
    "elif OPTIMIZER_NAME.lower() == 'sgd':\n",
    "    opt_ft = SGD(learning_rate=LR_PHASE_2, momentum=0.9)\n",
    "elif OPTIMIZER_NAME.lower() == 'adamw':\n",
    "    opt_ft = AdamW(learning_rate=LR_PHASE_2)\n",
    "else:\n",
    "    opt_ft = RMSprop(learning_rate=LR_PHASE_2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt_ft, \n",
    "    loss=FocalLoss(alpha=1.0, gamma=2.0), \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history_2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=len(history_1.history['loss']),\n",
    "    epochs=total_epochs,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=active_class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "p2_best_val_acc = max(history_2.history['val_accuracy'])\n",
    "print(f\"\\n‚úì Phase 2 Selesai!\")\n",
    "print(f\"‚úì Best Validation Accuracy Phase 2: {p2_best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 6. EVALUASI DAN METRIK (OTOMATIS)\n",
    "# =================================================================================================\n",
    "print(\"\\n[INFO] Melakukan Evaluasi Menyeluruh...\")\n",
    "\n",
    "# Load best model\n",
    "try:\n",
    "    model.load_weights(MODEL_SAVE_PATH)\n",
    "    print(f\"‚úì Best model loaded from {MODEL_SAVE_PATH}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Using current model weights\")\n",
    "\n",
    "# Prediksi (menghasilkan probabilitas, misal [0.1, 0.8, 0.05, 0.05])\n",
    "Y_pred_probs = model.predict(test_ds, verbose=1)\n",
    "\n",
    "# Ambil kelas dengan probabilitas tertinggi\n",
    "y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "\n",
    "# Label asli (Ground Truth)\n",
    "y_true_onehot = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "y_true = np.argmax(y_true_onehot, axis=1)\n",
    "\n",
    "class_names = ['Kelas 1', 'Kelas 2', 'Kelas 3', 'Kelas 4']\n",
    "\n",
    "# --- Metrik 1: Akurasi Global ---\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# --- Metrik 2: Confusion Matrix ---\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_str = str(cm.tolist())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# --- Metrik 3: Confidence Statistics ---\n",
    "confidence_scores = np.max(Y_pred_probs, axis=1)\n",
    "avg_conf = np.mean(confidence_scores)\n",
    "min_conf = np.min(confidence_scores)\n",
    "std_conf = np.std(confidence_scores)\n",
    "\n",
    "# --- Metrik 4: Precision, Recall, F1 Per Kelas ---\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, digits=4, output_dict=True)\n",
    "print(\"=\" * 60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "print(f\"‚úÖ Akurasi Test Akhir: {test_acc*100:.2f}%\")\n",
    "print(f\"‚úÖ Rata-rata Confidence: {avg_conf*100:.2f}%\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plot_folder = \"history_plots\" \n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder)\n",
    "\n",
    "current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plot_filename = f\"cm_resnet18_{current_time_str}_Acc{test_acc*100:.1f}.png\"\n",
    "plot_filepath = os.path.join(plot_folder, plot_filename)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            annot_kws={\"size\": 14})\n",
    "plt.title(f'Confusion Matrix - ResNet18 (Acc: {test_acc*100:.2f}%)')\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(plot_filepath, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nüñºÔ∏è Gambar Confusion Matrix tersimpan di: {plot_filepath}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7be1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================================\n",
    "# 7. SISTEM PENYIMPANAN LOG (AUTO-CSV)\n",
    "# =================================================================================================\n",
    "# Bagian ini yang akan menyimpan \"Jejak Percobaan\" kamu selamanya.\n",
    "\n",
    "# Dictionary data untuk satu baris CSV\n",
    "log_data = {\n",
    "    # Waktu\n",
    "    \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \n",
    "    # Model Info\n",
    "    \"Model\": \"ResNet18 (Custom)\",\n",
    "    \n",
    "    # Parameter Setup\n",
    "    \"Optimizer\": OPTIMIZER_NAME,\n",
    "    \"Batch_Size\": BATCH_SIZE,\n",
    "    \"Layer_Config\": str(LAYER_CONFIG),\n",
    "    \"Dropout\": DROPOUT_RATE,\n",
    "    \"Fine_Tune_At\": FINE_TUNE_AT,\n",
    "    \n",
    "    # Parameter Training\n",
    "    \"LR_Phase_1\": LR_PHASE_1,\n",
    "    \"Epochs_P1\": EPOCHS_PHASE_1,\n",
    "    \"LR_Phase_2\": LR_PHASE_2,\n",
    "    \"Epochs_P2\": EPOCHS_PHASE_2,\n",
    "    \"Label_Smoothing\": LABEL_SMOOTHING,\n",
    "    \"Use_Class_Weights\": USE_CLASS_WEIGHTS,\n",
    "    \n",
    "    # Hasil Evaluasi Global\n",
    "    \"Accuracy\": round(test_acc, 4),\n",
    "    \"Val_Acc_P1\": round(p1_best_val_acc, 4),\n",
    "    \"Val_Acc_P2\": round(p2_best_val_acc, 4),\n",
    "    \"Avg_Confidence\": round(avg_conf, 4),\n",
    "    \"Min_Confidence\": round(min_conf, 4),\n",
    "    \"Std_Confidence\": round(std_conf, 4),\n",
    "    \"Confusion_Matrix\": cm_str\n",
    "}\n",
    "\n",
    "# Loop otomatis untuk mengambil Precision/Recall/F1 tiap kelas\n",
    "for label in class_names:\n",
    "    metrics = report[label]\n",
    "    log_data[f\"Class_{label}_Prec\"] = round(metrics['precision'], 4)\n",
    "    log_data[f\"Class_{label}_Rec\"] = round(metrics['recall'], 4)\n",
    "    log_data[f\"Class_{label}_F1\"] = round(metrics['f1-score'], 4)\n",
    "\n",
    "# Buat DataFrame (1 baris)\n",
    "df_new_log = pd.DataFrame([log_data])\n",
    "\n",
    "# Logika Penyimpanan: Append jika file ada, Write jika belum ada\n",
    "if not os.path.exists(LOG_FILE_PATH):\n",
    "    df_new_log.to_csv(LOG_FILE_PATH, index=False)\n",
    "    print(f\"\\n[SAVE] File log baru dibuat: {LOG_FILE_PATH}\")\n",
    "else:\n",
    "    df_new_log.to_csv(LOG_FILE_PATH, mode='a', header=False, index=False)\n",
    "    print(f\"\\n[SAVE] Hasil percobaan berhasil ditambahkan ke: {LOG_FILE_PATH}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROSES SELESAI. SILAKAN UBAH PARAMETER DI ATAS UNTUK PERCOBAAN BERIKUTNYA.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f6f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
