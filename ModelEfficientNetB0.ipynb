{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-main",
   "metadata": {},
   "source": [
    "# üß† MODEL EFFICIENTNET-B0 UNTUK KLASIFIKASI AFLATOKSIN\n",
    "\n",
    "## Mengapa EfficientNet-B0?\n",
    "\n",
    "| Aspek | ResNet50 | EfficientNet-B0 | Keuntungan |\n",
    "|-------|----------|-----------------|------------|\n",
    "| **Parameter** | 25.6 juta | 5.3 juta | 5x lebih kecil ‚Üí kurang overfitting |\n",
    "| **Top-1 ImageNet** | 76.0% | 77.1% | Lebih akurat dengan parameter lebih sedikit |\n",
    "| **FLOPS** | 4.1B | 0.39B | 10x lebih efisien |\n",
    "\n",
    "## Perubahan Utama dari ResNet50:\n",
    "\n",
    "1. **Arsitektur dasar**: `ResNet50` ‚Üí `EfficientNetB0`\n",
    "2. **Classifier head lebih sederhana**: `[512, 256, 64]` ‚Üí `[256]` (1 layer saja)\n",
    "3. **Fine-tune layer**: Layer 140+ ‚Üí Layer ~200+ (lebih sedikit layer di-unfreeze)\n",
    "4. **Learning rate lebih konservatif**: Karena model lebih kecil\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-preprocess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: LOAD PREPROCESSING\n",
    "# =============================================================================\n",
    "# Menjalankan file preprocess_efficientnet.ipynb untuk memuat dataset\n",
    "# Pastikan file tersebut ada di folder yang sama!\n",
    "# =============================================================================\n",
    "\n",
    "%run preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORT LIBRARIES UNTUK MODEL\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "# Model imports\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    GlobalAveragePooling2D, \n",
    "    Dropout, \n",
    "    Input,\n",
    "    BatchNormalization  # Opsional, untuk stabilitas training\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, AdamW, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, \n",
    "    ReduceLROnPlateau, \n",
    "    ModelCheckpoint\n",
    ")\n",
    "\n",
    "# Metrics imports\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-focal-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: FOCAL LOSS (OPSIONAL - UNTUK IMBALANCED DATA)\n",
    "# =============================================================================\n",
    "#\n",
    "# PENJELASAN FOCAL LOSS:\n",
    "# \n",
    "# Standard Cross-Entropy:\n",
    "#   CE(p) = -log(p)  dimana p adalah probabilitas kelas yang benar\n",
    "#\n",
    "# Focal Loss (Lin et al., 2017):\n",
    "#   FL(p) = -Œ± * (1-p)^Œ≥ * log(p)\n",
    "#\n",
    "# Parameter:\n",
    "#   - Œ± (alpha): Bobot untuk menyeimbangkan kelas (biasanya 1.0)\n",
    "#   - Œ≥ (gamma): \"Focusing parameter\" (biasanya 2.0)\n",
    "#     - gamma=0: sama dengan cross-entropy\n",
    "#     - gamma>0: mengurangi loss untuk sampel yang mudah (high p)\n",
    "#                sehingga model fokus pada sampel yang sulit (low p)\n",
    "#\n",
    "# KAPAN GUNAKAN FOCAL LOSS:\n",
    "#   - Dataset imbalanced (seperti kasus Anda)\n",
    "#   - Model terlalu \"confident\" pada kelas mayoritas\n",
    "#\n",
    "# KAPAN GUNAKAN CATEGORICAL CROSSENTROPY:\n",
    "#   - Dataset balanced\n",
    "#   - Sebagai baseline untuk perbandingan\n",
    "# =============================================================================\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Focal Loss untuk menangani class imbalance.\n",
    "    Referensi: Lin et al., \"Focal Loss for Dense Object Detection\", ICCV 2017\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, gamma=2.0, name='focal_loss'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Bobot balancing (default 1.0)\n",
    "            gamma: Focusing parameter (default 2.0)\n",
    "                   - Semakin besar gamma, semakin fokus pada sampel sulit\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip prediksi untuk mencegah log(0)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Hitung cross entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Hitung focal weight: (1 - p)^gamma\n",
    "        # p adalah probabilitas untuk kelas yang benar\n",
    "        pt = y_true * y_pred  # Ambil probabilitas kelas benar\n",
    "        pt = tf.reduce_sum(pt, axis=-1, keepdims=True)\n",
    "        focal_weight = tf.pow(1.0 - pt, self.gamma)\n",
    "        \n",
    "        # Gabungkan: Œ± * (1-p)^Œ≥ * CE\n",
    "        focal_loss = self.alpha * focal_weight * tf.reduce_sum(ce, axis=-1, keepdims=True)\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "\n",
    "print(\"FocalLoss class defined.\")\n",
    "print(\"  - Default alpha=1.0, gamma=2.0\")\n",
    "print(\"  - Gunakan untuk dataset imbalanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: KONFIGURASI EKSPERIMEN\n",
    "# =============================================================================\n",
    "\n",
    "# Layer Dense setelah GlobalAveragePooling\n",
    "DENSE_UNITS = 256\n",
    "DROPOUT_RATE = 0.5 # Semakin tinggi = regularisasi lebih kuat\n",
    "\n",
    "# Fine-tune dari layer ke berapa? EfficientNet-B0 punya ~237 layers\n",
    "# Untuk dataset kecil, unfreeze sedikit layer saja (misal mulai dari 200)\n",
    "FINE_TUNE_AT = 200 # Semakin tinggi angka = semakin sedikit layer yang di-unfreeze\n",
    "\n",
    "# TRAINING PHASE 1: Feature Extraction (Base Model Frozen)\n",
    "LR_PHASE_1 = 1e-3 \n",
    "EPOCHS_PHASE_1 = 20\n",
    "\n",
    "# TRAINING PHASE 2: Fine-Tuning (Sebagian Base Model Unfrozen)\n",
    "LR_PHASE_2 = 1e-5 # HARUS jauh lebih kecil untuk mencegah \"catastrophic forgetting\"\n",
    "EPOCHS_PHASE_2 = 30\n",
    "\n",
    "BATCH_SIZE = 32             # Jumlah gambar yang diproses sekali jalan.\n",
    "OPTIMIZER_NAME = 'Adam'     # Pilihan: 'Adam', 'SGD', 'RMSprop'.\n",
    "\n",
    "# LOSS FUNCTION\n",
    "LOSS_TYPE = 'focal'  # 'focal' atau 'crossentropy'\n",
    "LABEL_SMOOTHING = 0.1\n",
    "FOCAL_ALPHA = 1.0\n",
    "FOCAL_GAMMA = 2.0\n",
    "\n",
    "# Nama file untuk menyimpan model terbaik\n",
    "MODEL_SAVE_PATH = 'best_efficientnet_aflatoxin.keras'\n",
    "\n",
    "# File log untuk mencatat semua eksperimen\n",
    "LOG_FILE_PATH = 'experiment_log_efficientnet.csv'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PRINT KONFIGURASI\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"KONFIGURASI EKSPERIMEN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìê ARSITEKTUR:\")\n",
    "print(f\"   Dense Units: {DENSE_UNITS}\")\n",
    "print(f\"   Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"   Fine-tune dari layer: {FINE_TUNE_AT}\")\n",
    "print(f\"\\nüìà PHASE 1 (Feature Extraction):\")\n",
    "print(f\"   Learning Rate: {LR_PHASE_1}\")\n",
    "print(f\"   Epochs: {EPOCHS_PHASE_1}\")\n",
    "print(f\"\\nüìà PHASE 2 (Fine-Tuning):\")\n",
    "print(f\"   Learning Rate: {LR_PHASE_2}\")\n",
    "print(f\"   Epochs: {EPOCHS_PHASE_2}\")\n",
    "print(f\"\\nüéØ REGULARISASI:\")\n",
    "print(f\"   Label Smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"   Loss Type: {LOSS_TYPE}\")\n",
    "if LOSS_TYPE == 'focal':\n",
    "    print(f\"   Focal Alpha: {FOCAL_ALPHA}\")\n",
    "    print(f\"   Focal Gamma: {FOCAL_GAMMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: MEMBANGUN MODEL EFFICIENTNET-B0\n",
    "# =============================================================================\n",
    "\n",
    "def build_efficientnet_model():\n",
    "    \n",
    "    # 1. Load EfficientNet-B0 pre-trained\n",
    "    base_model = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_height, img_width, 3)\n",
    "    )\n",
    "    \n",
    "    # 2. Freeze base model (Phase 1)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # 3. Bangun classifier head\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(img_height, img_width, 3), name='input_layer')\n",
    "    \n",
    "    # Pass through EfficientNet\n",
    "    # training=False: BatchNorm layers dalam mode inference\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    # Mengubah output (7, 7, 1280) menjadi (1280,)\n",
    "    # Alternatif: GlobalMaxPooling2D atau Flatten\n",
    "    x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # Dense layer dengan ReLU activation\n",
    "    x = Dense(DENSE_UNITS, activation='relu', name='dense_1')(x)\n",
    "    \n",
    "    # Dropout untuk regularisasi\n",
    "    x = Dropout(DROPOUT_RATE, name='dropout')(x)\n",
    "    \n",
    "    # Output layer dengan Softmax\n",
    "    # 4 neuron untuk 4 kelas (1 PPB, 2 PPB, 3 PPB, 4 PPB)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. Buat model\n",
    "    # -------------------------------------------------------------------------\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='EfficientNetB0_Aflatoxin')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Bangun model\n",
    "model, base_model = build_efficientnet_model()\n",
    "\n",
    "# Tampilkan informasi model\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL BERHASIL DIBANGUN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "print(f\"Non-trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights]):,}\")\n",
    "print(f\"\\nBase model layers: {len(base_model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: COMPILE MODEL\n",
    "\n",
    "# Pilih loss function\n",
    "if LOSS_TYPE == 'focal':\n",
    "    loss_fn = FocalLoss(alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA)\n",
    "    loss_name = f\"Focal Loss (Œ±={FOCAL_ALPHA}, Œ≥={FOCAL_GAMMA})\"\n",
    "else:\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    )\n",
    "    loss_name = f\"Categorical Crossentropy (smoothing={LABEL_SMOOTHING})\"\n",
    "\n",
    "# Setup Optimizer Dinamis sesuai Konfigurasi\n",
    "if OPTIMIZER_NAME.lower() == 'Adam':\n",
    "    opt = Adam(learning_rate=LR_PHASE_1)\n",
    "elif OPTIMIZER_NAME.lower() == 'sgd':\n",
    "    opt = SGD(learning_rate=LR_PHASE_1)\n",
    "elif OPTIMIZER_NAME.lower() == 'AdamW':\n",
    "    opt = AdamW(learning_rate=LR_PHASE_1)\n",
    "else:\n",
    "    opt = RMSprop(learning_rate=LR_PHASE_1)\n",
    "    \n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled!\")\n",
    "print(f\"  - Optimizer: {OPTIMIZER_NAME} (lr={LR_PHASE_1})\")\n",
    "print(f\"  - Loss: {loss_name}\")\n",
    "print(f\"  - Metrics: accuracy\")\n",
    "print(f\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-callbacks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: SETUP CALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  ‚úì EarlyStopping (patience=10, monitor=val_accuracy)\")\n",
    "print(\"  ‚úì ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
    "print(f\"  ‚úì ModelCheckpoint (save to: {MODEL_SAVE_PATH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-phase1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: PHASE 1 - FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "#\n",
    "# TUJUAN: Melatih classifier head sementara base model dibekukan\n",
    "#\n",
    "# MENGAPA 2 PHASE?\n",
    "# 1. Phase 1: Classifier head belajar menggunakan fitur dari EfficientNet\n",
    "#    - Base model sudah punya fitur bagus dari ImageNet\n",
    "#    - Kita hanya perlu melatih bagaimana menggunakan fitur tersebut\n",
    "#\n",
    "# 2. Phase 2: Fine-tune sebagian base model\n",
    "#    - Setelah head sudah bagus, kita \"tune\" fitur untuk domain spesifik\n",
    "#    - LR sangat kecil agar tidak merusak fitur yang sudah bagus\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase model: FROZEN\")\n",
    "print(f\"Training: Classifier head only\")\n",
    "print(f\"Learning rate: {LR_PHASE_1}\")\n",
    "print(f\"Epochs: {EPOCHS_PHASE_1}\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_PHASE_1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Simpan hasil Phase 1\n",
    "best_val_acc_p1 = max(history_phase1.history['val_accuracy'])\n",
    "print(f\"\\n‚úì Phase 1 Complete!\")\n",
    "print(f\"‚úì Best Validation Accuracy: {best_val_acc_p1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-unfreeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: UNFREEZE BASE MODEL UNTUK FINE-TUNING\n",
    "# =============================================================================\n",
    "#\n",
    "# STRATEGI FINE-TUNING:\n",
    "#\n",
    "# EfficientNet-B0 memiliki ~237 layers\n",
    "# Layer awal: mendeteksi fitur umum (edges, textures)\n",
    "# Layer akhir: mendeteksi fitur spesifik (patterns, objects)\n",
    "#\n",
    "# Untuk dataset kecil:\n",
    "# - Freeze layer awal (fitur umum sudah bagus)\n",
    "# - Unfreeze hanya beberapa layer akhir\n",
    "# - FINE_TUNE_AT = 200 berarti layer 0-199 tetap frozen\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPARING PHASE 2: FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze layer awal, unfreeze layer akhir\n",
    "for layer in base_model.layers[:FINE_TUNE_AT]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Hitung statistik\n",
    "total_layers = len(base_model.layers)\n",
    "frozen_layers = FINE_TUNE_AT\n",
    "trainable_layers = total_layers - FINE_TUNE_AT\n",
    "\n",
    "print(f\"\\nBase model layers: {total_layers}\")\n",
    "print(f\"Frozen layers: {frozen_layers} (0 to {FINE_TUNE_AT-1})\")\n",
    "print(f\"Trainable layers: {trainable_layers} ({FINE_TUNE_AT} to {total_layers-1})\")\n",
    "\n",
    "if OPTIMIZER_NAME.lower() == 'Adam':\n",
    "    opt_ft = Adam(learning_rate=LR_PHASE_2)\n",
    "elif OPTIMIZER_NAME.lower() == 'sgd':\n",
    "    opt_ft = SGD(learning_rate=LR_PHASE_2)\n",
    "elif OPTIMIZER_NAME.lower() == 'AdamW':\n",
    "    opt_ft = AdamW(learning_rate=LR_PHASE_2)\n",
    "else:\n",
    "    opt_ft = RMSprop(learning_rate=LR_PHASE_2)\n",
    "    \n",
    "# Re-compile dengan learning rate lebih kecil\n",
    "# PENTING: Harus re-compile setelah mengubah trainable status\n",
    "model.compile(\n",
    "    optimizer=opt_ft,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Model re-compiled with LR={LR_PHASE_2}\")\n",
    "print(f\"\\nTrainable parameters now: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-phase2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: PHASE 2 - FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 2: FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase model: PARTIALLY UNFROZEN (from layer {FINE_TUNE_AT})\")\n",
    "print(f\"Learning rate: {LR_PHASE_2}\")\n",
    "print(f\"Epochs: {EPOCHS_PHASE_2}\")\n",
    "print(\"\\nStarting fine-tuning...\\n\")\n",
    "\n",
    "# Lanjutkan dari epoch terakhir Phase 1\n",
    "initial_epoch = len(history_phase1.history['loss'])\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=initial_epoch,\n",
    "    epochs=initial_epoch + EPOCHS_PHASE_2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Simpan hasil Phase 2\n",
    "best_val_acc_p2 = max(history_phase2.history['val_accuracy'])\n",
    "print(f\"\\n‚úì Phase 2 Complete!\")\n",
    "print(f\"‚úì Best Validation Accuracy: {best_val_acc_p2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5565d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 5: Evaluation on data set...\")\n",
    "\n",
    "# Load model terbaik\n",
    "model = tf.keras.models.load_model('best_efficientnet_aflatoxin.keras')\n",
    "\n",
    "# Evaluasi\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
    "print(f\"\\n‚úì Test Loss: {test_loss:.4f}\")\n",
    "print(f\"‚úì Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: EVALUASI PADA TEST SET\n",
    "# =============================================================================\n",
    "# Prediksi\n",
    "print(\"\\nMelakukan prediksi...\")\n",
    "\n",
    "print(f\"\\nTEST ACCURACY: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 12: VISUALISASI TRAINING HISTORY\n",
    "# =============================================================================\n",
    "y_pred_probs = model.predict(test_ds, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Ground truth\n",
    "y_true_onehot = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "y_true = np.argmax(y_true_onehot, axis=1)\n",
    "\n",
    "# Nama kelas untuk report\n",
    "target_names = [f'Kelas {name})' for name in class_names]\n",
    "\n",
    "# Hitung metrik\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gabungkan history dari kedua phase\n",
    "acc = history_phase1.history['accuracy'] + history_phase2.history['accuracy']\n",
    "val_acc = history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy']\n",
    "loss = history_phase1.history['loss'] + history_phase2.history['loss']\n",
    "val_loss = history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "\n",
    "epochs_range = range(1, len(acc) + 1)\n",
    "phase1_end = len(history_phase1.history['accuracy'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[0].plot(epochs_range, acc, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(epochs_range, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[0].axvline(x=phase1_end, color='green', linestyle='--', label='Phase 1 ‚Üí 2')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Loss\n",
    "axes[1].plot(epochs_range, loss, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[1].plot(epochs_range, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[1].axvline(x=phase1_end, color='green', linestyle='--', label='Phase 1 ‚Üí 2')\n",
    "axes[1].set_title('Model Loss', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Siapkan Folder Penyimpanan Gambar\n",
    "plot_folder = \"visualizations\" \n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder) # Buat folder otomatis jika belum ada\n",
    "# Buat Nama File Unik (Timestamp + Akurasi)\n",
    "current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plot_filename = f\"cm_{current_time_str}_Acc{test_accuracy*100:.1f}.png\"\n",
    "plotfilepath = os.path.join(plot_folder, plot_filename)\n",
    "\n",
    "plt.savefig(plotfilepath, dpi=300, bbox_inches='tight')\n",
    "plt.title(f'Visualisasi Hasil (Acc: {test_accuracy*100:.2f}%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: CLASSIFICATION REPORT & CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=target_names, \n",
    "    digits=4,\n",
    "    output_dict=True\n",
    ")\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Siapkan Folder Penyimpanan Gambar\n",
    "plot_folder = \"history_plots_ENB0\" \n",
    "if not os.path.exists(plot_folder):\n",
    "    os.makedirs(plot_folder) # Buat folder otomatis jika belum ada\n",
    "# Buat Nama File Unik (Timestamp + Akurasi)\n",
    "current_time_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plot_filename = f\"cm_{current_time_str}_Acc{test_accuracy*100:.1f}.png\"\n",
    "plot_filepath = os.path.join(plot_folder, plot_filename)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=target_names,\n",
    "    yticklabels=target_names,\n",
    "    annot_kws={'size': 14}\n",
    ")\n",
    "plt.title(f'Confusion Matrix - EfficientNet-B0\\n(Test Accuracy: {test_accuracy*100:.2f}%)', fontsize=14)\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Simpan gambar\n",
    "plt.savefig(plot_filepath, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Confusion matrix saved to: {plot_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: ANALISIS CONFIDENCE\n",
    "# =============================================================================\n",
    "#\n",
    "# Confidence = seberapa \"yakin\" model dengan prediksinya\n",
    "# High confidence + correct = bagus\n",
    "# High confidence + wrong = berbahaya (overconfident)\n",
    "# Low confidence = model ragu-ragu (mungkin data ambigu)\n",
    "# =============================================================================\n",
    "\n",
    "# Hitung confidence untuk setiap prediksi\n",
    "confidence_scores = np.max(y_pred_probs, axis=1)\n",
    "\n",
    "# Statistik\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "min_confidence = np.min(confidence_scores)\n",
    "max_confidence = np.max(confidence_scores)\n",
    "std_confidence = np.std(confidence_scores)\n",
    "\n",
    "# Confidence untuk prediksi benar vs salah\n",
    "correct_mask = (y_pred == y_true)\n",
    "correct_confidence = np.mean(confidence_scores[correct_mask])\n",
    "wrong_confidence = np.mean(confidence_scores[~correct_mask]) if (~correct_mask).sum() > 0 else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALISIS CONFIDENCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nStatistik Overall:\")\n",
    "print(f\"  - Average Confidence: {avg_confidence*100:.2f}%\")\n",
    "print(f\"  - Min Confidence: {min_confidence*100:.2f}%\")\n",
    "print(f\"  - Max Confidence: {max_confidence*100:.2f}%\")\n",
    "print(f\"  - Std Confidence: {std_confidence*100:.2f}%\")\n",
    "print(f\"\\nConfidence by Prediction:\")\n",
    "print(f\"  - Correct predictions: {correct_confidence*100:.2f}%\")\n",
    "print(f\"  - Wrong predictions: {wrong_confidence*100:.2f}%\")\n",
    "\n",
    "# Interpretasi\n",
    "if correct_confidence > wrong_confidence + 0.1:\n",
    "    print(\"\\n‚úì Model lebih confident pada prediksi yang benar (bagus!)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Model cukup confident bahkan pada prediksi salah (hati-hati overconfidence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-log",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: SIMPAN LOG EKSPERIMEN\n",
    "# =============================================================================\n",
    "\n",
    "# Buat dictionary dengan semua informasi eksperimen\n",
    "log_data = {\n",
    "    # Timestamp\n",
    "    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \n",
    "    # Arsitektur\n",
    "    \"Optimizer\": OPTIMIZER_NAME,\n",
    "    \"Batch_Size\": BATCH_SIZE,\n",
    "    'Dense_Units': DENSE_UNITS,\n",
    "    'Dropout': DROPOUT_RATE,\n",
    "    'Fine_Tune_At': FINE_TUNE_AT,\n",
    "    \n",
    "    # Training params\n",
    "    'LR_Phase_1': LR_PHASE_1,\n",
    "    'Epochs_P1': EPOCHS_PHASE_1,\n",
    "    'LR_Phase_2': LR_PHASE_2,\n",
    "    'Epochs_P2': EPOCHS_PHASE_2,\n",
    "    \n",
    "    # Regularisasi\n",
    "    'Loss_Type': LOSS_TYPE,\n",
    "    'Label_Smoothing': LABEL_SMOOTHING if LOSS_TYPE == 'crossentropy' else 'N/A',\n",
    "    'Focal_Gamma': FOCAL_GAMMA if LOSS_TYPE == 'focal' else 'N/A',\n",
    "    \n",
    "    # Hasil\n",
    "    'Accuracy': round(test_accuracy, 4),\n",
    "    'Avg_Confidence': round(avg_confidence, 4),\n",
    "    'Confusion_Matrix': str(cm.tolist()),\n",
    "}\n",
    "\n",
    "# Tambahkan metrik per kelas\n",
    "for name in target_names:\n",
    "    log_data[f'{name}_Precision'] = round(report[name]['precision'], 4)\n",
    "    log_data[f'{name}_Recall'] = round(report[name]['recall'], 4)\n",
    "    log_data[f'{name}_F1'] = round(report[name]['f1-score'], 4)\n",
    "\n",
    "# Simpan ke CSV\n",
    "df_log = pd.DataFrame([log_data])\n",
    "\n",
    "if os.path.exists(LOG_FILE_PATH):\n",
    "    df_log.to_csv(LOG_FILE_PATH, mode='a', header=False, index=False)\n",
    "    print(f\"‚úì Log ditambahkan ke: {LOG_FILE_PATH}\")\n",
    "else:\n",
    "    df_log.to_csv(LOG_FILE_PATH, index=False)\n",
    "    print(f\"‚úì File log baru dibuat: {LOG_FILE_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EKSPERIMEN SELESAI\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Model tersimpan di: {MODEL_SAVE_PATH}\")\n",
    "print(f\"üìÅ Log tersimpan di: {LOG_FILE_PATH}\")\n",
    "print(f\"\\nüéØ Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-next-steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Langkah Selanjutnya\n",
    "\n",
    "### Jika akurasi belum memuaskan, coba:\n",
    "\n",
    "1. **Ubah FINE_TUNE_AT**: \n",
    "   - Lebih kecil (misal 150) = lebih banyak layer di-train = lebih adaptif tapi risiko overfitting\n",
    "   - Lebih besar (misal 220) = lebih sedikit layer = lebih stable\n",
    "\n",
    "2. **Ubah DENSE_UNITS**:\n",
    "   - Coba [128] atau [512]\n",
    "   \n",
    "3. **Ubah DROPOUT_RATE**:\n",
    "   - Naikan ke 0.6 jika overfitting\n",
    "   - Turunkan ke 0.3 jika underfitting\n",
    "\n",
    "4. **Coba loss function lain**:\n",
    "   - Ubah LOSS_TYPE ke 'crossentropy' dengan LABEL_SMOOTHING=0.2\n",
    "\n",
    "5. **Coba arsitektur lain**:\n",
    "   - EfficientNet-B1 (sedikit lebih besar)\n",
    "   - DenseNet-121\n",
    "   - MobileNetV2\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
