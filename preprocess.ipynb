{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02855991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pathlib\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b2b665db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 507 files belonging to 10 classes.\n",
      "['0.5', '1', '1.5', '2', '2.5', '3', '3.5', '4', '5', '6']\n"
     ]
    }
   ],
   "source": [
    "#path dataset dan parameter\n",
    "# data_dir = pathlib.Path(r\"C:\\Users\\PC\\Documents\\innar\\data akhir valid merah\")\n",
    "data_dir = pathlib.Path(r\"D:\\SKRIPSI\\kode\\data akhir valid merah\")\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "\n",
    "# Membaca dataset (label dari nama folder)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"grayscale\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87b624b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAHAP PREPROCESSING\n",
    "# Enhancement CLAHE\n",
    "def apply_clahe_np(img):\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(img)\n",
    "    return enhanced.astype(np.float32) / 255.0\n",
    "\n",
    "# Augmentasi data\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.05),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d21bad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filename  label_ppb\n",
      "0  m1,5-46.png        1.5\n",
      "1    m2-33.png        2.0\n",
      "2     m4-9.png        4.0\n",
      "3    m1-44.png        1.0\n",
      "4    m3-57.png        3.0\n"
     ]
    }
   ],
   "source": [
    "#MENJADIKAN DATA TABULAR\n",
    "#ambil nama file dari dataset\n",
    "file_paths = list(train_ds.file_paths)\n",
    "\n",
    "#list untuk menyimpan hasil tabular\n",
    "data_list = []\n",
    "\n",
    "#loop per item dataset\n",
    "for i, (img_batch, label_batch) in enumerate(train_ds):\n",
    "    img = img_batch[0].numpy().squeeze() / 255.0  # Normalisasi\n",
    "    clahe_img = apply_clahe_np(img)\n",
    "    clahe_img = np.expand_dims(clahe_img, axis=-1)\n",
    "    aug_img = data_augmentation(tf.convert_to_tensor(clahe_img[None, ...]))[0].numpy().squeeze()\n",
    "    \n",
    "    # Nama file asli\n",
    "    file_name = os.path.basename(file_paths[i])\n",
    "    # Ambil nama folder (label ppb) dari path file\n",
    "    label_ppb = float(os.path.basename(os.path.dirname(file_paths[i])))\n",
    "\n",
    "    data_list.append({\n",
    "        \"filename\": file_name,\n",
    "        \"label_ppb\": label_ppb,\n",
    "        # \"image_array\": img  # atau ganti dengan img / clahe_img jika mau simpan hasil lain\n",
    "    })\n",
    "\n",
    "#konversi ke dataframe\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "#tampilkan ringkasan\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dcf05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(224, 224, 3),  # HARUS RGB\n",
    "    include_top=False, \n",
    "    weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script pembagian data: 70% train, 20% validasi, 10% test\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_folder(source_dir, dest_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    source_dir = pathlib.Path(source_dir)\n",
    "    dest_dir = pathlib.Path(dest_dir)\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_folder in source_dir.iterdir():\n",
    "        if not class_folder.is_dir():\n",
    "            continue\n",
    "        files = list(class_folder.glob('*.png'))\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        train_files, temp_files = train_test_split(files, train_size=train_ratio, random_state=42)\n",
    "        val_size = val_ratio / (val_ratio + test_ratio)\n",
    "        val_files, test_files = train_test_split(temp_files, train_size=val_size, random_state=42)\n",
    "\n",
    "        for split_name, split_files in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n",
    "            split_class_dir = dest_dir / split_name / class_folder.name\n",
    "            split_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "            for f in split_files:\n",
    "                shutil.copy(str(f), str(split_class_dir / f.name))\n",
    "\n",
    "# Contoh penggunaan:\n",
    "split_data_folder(\n",
    "    # source_dir=r'C:\\Users\\PC\\Documents\\innar\\data akhir valid merah',\n",
    "    # dest_dir=r'C:\\Users\\PC\\Documents\\innar\\data_split_merah',\n",
    "    source_dir=r'D:\\SKRIPSI\\kode\\data akhir valid merah',\n",
    "    dest_dir=r'D:\\SKRIPSI\\kode\\data_split_merah',\n",
    "    train_ratio=0.7, val_ratio=0.2, test_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a7a93959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "data_split_dir = pathlib.Path(r\"D:\\SKRIPSI\\kode\\data_split_merah\")\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "print(\"Loading datasets...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4245ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 349 files belonging to 10 classes.\n",
      "Found 104 files belonging to 10 classes.\n",
      "Found 54 files belonging to 10 classes.\n",
      "Classes found: ['0.5', '1', '1.5', '2', '2.5', '3', '3.5', '4', '5', '6']\n",
      "Class to PPB mapping: {0: 0.5, 1: 1.0, 2: 1.5, 3: 2.0, 4: 2.5, 5: 3.0, 6: 3.5, 7: 4.0, 8: 5.0, 9: 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Load train dataset\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_split_dir / 'train',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",  # RGB untuk ResNet-50\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_split_dir / 'val',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\", \n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_split_dir / 'test',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\", \n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Classes found: {class_names}\")\n",
    "\n",
    "# Convert class names to PPB values for regression\n",
    "class_to_ppb = {i: float(name) for i, name in enumerate(class_names)}\n",
    "print(f\"Class to PPB mapping: {class_to_ppb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4519b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Converting labels to PPB values...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: Converting labels to PPB values...\")\n",
    "\n",
    "# Function to convert integer labels to float PPB values\n",
    "def convert_labels_to_ppb(images, labels):\n",
    "    ppb_labels = tf.cast(labels, tf.float32)\n",
    "    for class_idx, ppb_val in class_to_ppb.items():\n",
    "        ppb_labels = tf.where(labels == class_idx, ppb_val, ppb_labels)\n",
    "    return images, ppb_labels\n",
    "\n",
    "# Apply label conversion\n",
    "train_ds = train_ds.map(convert_labels_to_ppb, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(convert_labels_to_ppb, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(convert_labels_to_ppb, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1448d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Setting up data augmentation...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Setting up data augmentation...\")\n",
    "\n",
    "# Data augmentation using Keras layers (TensorFlow 2.x compatible)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),  # 0.1 radians ≈ 5.7 degrees\n",
    "    tf.keras.layers.RandomBrightness(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# Preprocessing function with Keras augmentation\n",
    "def preprocess_with_augmentation(images, labels, training=False):\n",
    "    # Normalize images to [0, 1] range\n",
    "    images = tf.cast(images, tf.float32) / 255.0\n",
    "    \n",
    "    if training:\n",
    "        # Apply augmentation using Keras layers\n",
    "        images = data_augmentation(images, training=True)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Apply preprocessing\n",
    "train_ds = train_ds.map(lambda x, y: preprocess_with_augmentation(x, y, training=True), \n",
    "                       num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: preprocess_with_augmentation(x, y, training=False), \n",
    "                   num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(lambda x, y: preprocess_with_augmentation(x, y, training=False), \n",
    "                     num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2336fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Optimizing data pipeline...\n",
      "Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Optimizing data pipeline...\")\n",
    "\n",
    "# Optimize performance with caching and prefetching\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2689d8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Building ResNet-50 model...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 0us/step\n",
      "Model compiled successfully!\n",
      "Total parameters: 24,784,641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AflatoxinEstimator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"AflatoxinEstimator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_512 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_256 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aflatoxin_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_512 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_256 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ aflatoxin_output (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,784,641</span> (94.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,784,641\u001b[0m (94.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,196,929</span> (4.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,196,929\u001b[0m (4.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Step 5: Building ResNet-50 model...\")\n",
    "\n",
    "# Base ResNet-50 model (pre-trained on ImageNet)\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',  # Use pre-trained weights\n",
    "    include_top=False,   # Exclude top classification layer\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze base model initially for transfer learning\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the complete model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu', name='dense_512'),\n",
    "    layers.Dropout(0.5, name='dropout_1'),\n",
    "    layers.Dense(256, activation='relu', name='dense_256'),\n",
    "    layers.Dropout(0.3, name='dropout_2'),\n",
    "    layers.Dense(64, activation='relu', name='dense_64'),\n",
    "    layers.Dense(1, activation='linear', name='aflatoxin_output')  # Regression output\n",
    "], name='AflatoxinEstimator')\n",
    "\n",
    "# Compile model for regression\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='mse',  # Mean Squared Error for regression\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f1ec94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Setting up training callbacks...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: Setting up training callbacks...\")\n",
    "\n",
    "# Training callbacks for better training control\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_aflatoxin_resnet50.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "86961321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Phase 1 - Training with frozen base model...\n",
      "Epoch 1/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.4107 - mae: 1.2631 - mse: 2.4107\n",
      "Epoch 1: val_loss improved from None to 0.44821, saving model to best_aflatoxin_resnet50.keras\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - loss: 1.5402 - mae: 0.9788 - mse: 1.5402 - val_loss: 0.4482 - val_mae: 0.5572 - val_mse: 0.4482 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.9268 - mae: 0.7636 - mse: 0.9268\n",
      "Epoch 2: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.9091 - mae: 0.7629 - mse: 0.9091 - val_loss: 0.5370 - val_mae: 0.5469 - val_mse: 0.5370 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.6804 - mae: 0.6460 - mse: 0.6804\n",
      "Epoch 3: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - loss: 0.7400 - mae: 0.6695 - mse: 0.7400 - val_loss: 0.7437 - val_mae: 0.6588 - val_mse: 0.7437 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.6736 - mae: 0.6379 - mse: 0.6736\n",
      "Epoch 4: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - loss: 0.6136 - mae: 0.6105 - mse: 0.6136 - val_loss: 0.5949 - val_mae: 0.5848 - val_mse: 0.5949 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.7281 - mae: 0.6706 - mse: 0.7281\n",
      "Epoch 5: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 0.6899 - mae: 0.6695 - mse: 0.6899 - val_loss: 0.5401 - val_mae: 0.5492 - val_mse: 0.5401 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5930 - mae: 0.6176 - mse: 0.5930\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - loss: 0.5785 - mae: 0.6010 - mse: 0.5785 - val_loss: 0.6059 - val_mae: 0.5913 - val_mse: 0.6059 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.4725 - mae: 0.5381 - mse: 0.4725\n",
      "Epoch 7: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - loss: 0.5401 - mae: 0.5730 - mse: 0.5401 - val_loss: 0.5798 - val_mae: 0.5757 - val_mse: 0.5798 - learning_rate: 5.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.6109 - mae: 0.6007 - mse: 0.6109\n",
      "Epoch 8: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - loss: 0.5838 - mae: 0.6058 - mse: 0.5838 - val_loss: 0.5905 - val_mae: 0.5823 - val_mse: 0.5905 - learning_rate: 5.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.5292 - mae: 0.5838 - mse: 0.5292\n",
      "Epoch 9: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - loss: 0.6230 - mae: 0.6235 - mse: 0.6230 - val_loss: 0.5770 - val_mae: 0.5740 - val_mse: 0.5770 - learning_rate: 5.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 0.5251 - mae: 0.5696 - mse: 0.5251\n",
      "Epoch 10: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - loss: 0.5561 - mae: 0.5885 - mse: 0.5561 - val_loss: 0.5481 - val_mae: 0.5549 - val_mse: 0.5481 - learning_rate: 5.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5476 - mae: 0.5745 - mse: 0.5476\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.44821\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 0.5605 - mae: 0.5751 - mse: 0.5605 - val_loss: 0.5578 - val_mae: 0.5615 - val_mse: 0.5578 - learning_rate: 5.0000e-05\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: Phase 1 - Training with frozen base model...\")\n",
    "\n",
    "# Phase 1: Train with frozen base model\n",
    "initial_epochs = 20\n",
    "\n",
    "history_1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=initial_epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45d7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
