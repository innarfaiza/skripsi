{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01caa701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Splitting data...\n",
      "Loading datasets...\n",
      "Found 366 files belonging to 3 classes.\n",
      "Found 61 files belonging to 3 classes.\n",
      "Found 32 files belonging to 3 classes.\n",
      "\n",
      "Classes found: ['1', '2', '3']\n",
      "Class to PPB mapping: {0: 1.0, 1: 2.0, 2: 3.0}\n",
      "Step 2: Converting labels to PPB values...\n",
      "Step 3: Setting up data augmentation...\n",
      "Step 4: Optimizing data pipeline...\n",
      "Data preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import keras_tuner as kt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%run preprocess.ipynb\n",
    "\n",
    "from training_history import enable_auto_export_csv\n",
    "\n",
    "# export setiap run ke file 'training_history.csv' (set ke 1 untuk export langsung)\n",
    "enable_auto_export_csv(1, 'training_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d920cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Setting up training callbacks...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: Setting up training callbacks...\")\n",
    "\n",
    "# Training callbacks for better training control\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=15, #jumlah epoch yg ditunggu\n",
    "        restore_best_weights=True, \n",
    "        verbose=1 #menampilkan pesan di konsol saat pelatihan dihentikan\n",
    "    ), # Stop training if no improvement in validation loss\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5, #kurangi LR 50%\n",
    "        patience=10,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ), # Reduce learning rate if no improvement\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_aflatoxin_resnet50.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    ) # Save best model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e076d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisikan Global Variables yang akan digunakan di model.fit\n",
    "INITIAL_EPOCHS = 25 \n",
    "FINE_TUNE_EPOCHS = 30 \n",
    "AUTOTUNE = tf.data.AUTOTUNE # Asumsi AUTOTUNE didefinisikan di preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07c961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from aflatoxin_tuning_results\\ResNet_Aflatoxin_Reg\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Fungsi Pembangunan Model untuk Keras Tuner (Menggantikan Kode Model Asli)\n",
    "\n",
    "def build_model_for_tuning(hp):\n",
    "    # --- 1. DEFENISI BASE MODEL ---\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # 2. Setup FREEZE/UNFREEZE\n",
    "    # TUNE: Fine-Tuning Depth (Lapisan Terdalam ResNet)\n",
    "    fine_tune_at = hp.Choice('fine_tune_at', values=[170, 140]) \n",
    "    \n",
    "    # Set semua trainable=True dulu, lalu bekukan yang awal\n",
    "    base_model.trainable = True \n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # --- 3. PEMBANGUNAN HEAD LAYER ---\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # TUNE: Jumlah Neuron di Lapisan Dense Pertama (L1) - Simplifikasi Model\n",
    "    hp_units_l1 = hp.Choice('units_l1', values=[128, 64, 32])\n",
    "    x = layers.Dense(units=hp_units_l1, activation='relu', name='hp_dense_1')(x)\n",
    "    \n",
    "    # TUNE: Dropout Rate\n",
    "    hp_dropout = hp.Float('dropout_rate', min_value=0.2, max_value=0.4, step=0.1)\n",
    "    x = layers.Dropout(rate=hp_dropout, name='hp_dropout')(x)\n",
    "    \n",
    "    # Tambahkan Dense Layer 2 (opsional, tetapi tetap kecil)\n",
    "    x = layers.Dense(32, activation='relu', name='dense_32_fixed')(x) \n",
    "    \n",
    "    # Lapisan Regresi Akhir (Tetap)\n",
    "    outputs = layers.Dense(1, activation='linear', name='aflatoxin_output')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='HP_Estimator')\n",
    "\n",
    "    # --- 4. KOMPILASI MODEL (Menggunakan LR Tuning) ---\n",
    "    \n",
    "    # TUNE: Learning Rate Phase 2\n",
    "    hp_lr = hp.Choice('learning_rate', values=[1e-7, 5e-7, 1e-6, 5e-6]) \n",
    "    \n",
    "    # TUNE: Optimizer\n",
    "    hp_optimizer_name = hp.Choice('optimizer', values=['Adam', 'RMSprop'])\n",
    "    \n",
    "    if hp_optimizer_name == 'RMSprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp_lr)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_lr)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Model Checkpoint untuk Keras Tuner (Hanya simpan yang terbaik)\n",
    "tuner_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mae', # Fokus pada MAE (lebih mudah diinterpretasikan)\n",
    "        patience=10, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Inisialisasi Keras Tuner (Random Search)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model_for_tuning,\n",
    "    objective='val_mae', # Target: Meminimalkan Mean Absolute Error pada data validasi\n",
    "    max_trials=30,       # Coba 30 kombinasi parameter yang berbeda\n",
    "    executions_per_trial=1, # Tiap kombinasi dilatih 1 kali\n",
    "    directory='aflatoxin_tuning_results',\n",
    "    project_name='ResNet_Aflatoxin_Reg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678ad9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 06m 46s]\n",
      "val_mae: 0.553886353969574\n",
      "\n",
      "Best val_mae So Far: 0.553886353969574\n",
      "Total elapsed time: 02h 59m 08s\n",
      "\n",
      "Step 8: Getting the best model...\n",
      "\n",
      "--- HASIL HYPERPARAMETER TERBAIK ---\n",
      "{'fine_tune_at': 140, 'units_l1': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 5e-06, 'optimizer': 'RMSprop'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 50 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 & 4 (DIGABUNG dan DIUBAH total)\n",
    "\n",
    "# 1. Pastikan model dengan LR 1e-6 sudah dilatih dan disimpan\n",
    "# ASUMSI: File 'best_aflatoxin_resnet50.keras' berisi bobot terbaik Fase 1 (Frozen Base)\n",
    "\n",
    "# 2. Muat Bobot Terbaik FASE 1\n",
    "# Muat model terbaik Fase 1 (Frozen Base) untuk dijadikan bobot awal pada semua trial RS.\n",
    "# Ini penting karena RS harus mulai dari titik terbaik yang sudah stabil.\n",
    "best_frozen_model = tf.keras.models.load_model('best_aflatoxin_resnet50.keras')\n",
    "initial_weights = best_frozen_model.get_weights()\n",
    "\n",
    "\n",
    "print(\"Step 7: Starting Random Search for Fine-Tuning Hyperparameters...\")\n",
    "\n",
    "# --- JALANKAN SEARCH ---\n",
    "# Kita menjalankan RS dengan epochs total Phase 2 (Fine-Tuning)\n",
    "tuner.search(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=FINE_TUNE_EPOCHS, \n",
    "    callbacks=tuner_callbacks,\n",
    "    # Menggunakan Initial Weights dari Fase 1 untuk semua trial:\n",
    "    # Keras Tuner akan menggunakan weights ini untuk inisialisasi pada build_model_for_tuning\n",
    "    # Namun, karena tidak ada argumen `initial_weights` di tuner.search, \n",
    "    # kita harus memodifikasi fungsi build_model_for_tuning untuk memuatnya secara eksplisit\n",
    "    # (Ini membutuhkan penyesuaian yang kompleks, jadi kita fokus pada tuning LR/Dropout/Unit)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nStep 8: Getting the best model...\")\n",
    "# Ambil model terbaik berdasarkan MAE Validasi\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(\"\\n--- HASIL HYPERPARAMETER TERBAIK ---\")\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df14bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: Analyzing Random Search Results...\n",
      "\n",
      "--- HYPERPARAMETER TERBAIK DITEMUKAN ---\n",
      "LR: 5.0e-06\n",
      "Dropout Rate: 0.3\n",
      "Units L1: 128\n",
      "Fine Tune At: 140\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 (DIGANTI TOTAL, Menjadi Post-Tuning Analysis)\n",
    "\n",
    "print(\"Step 9: Analyzing Random Search Results...\")\n",
    "\n",
    "# Ambil model terbaik berdasarkan MAE Validasi (setelah Random Search selesai)\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "print(\"\\n--- HYPERPARAMETER TERBAIK DITEMUKAN ---\")\n",
    "print(f\"LR: {best_hp.get('learning_rate'):.1e}\")\n",
    "print(f\"Dropout Rate: {best_hp.get('dropout_rate'):.1f}\")\n",
    "print(f\"Units L1: {best_hp.get('units_l1')}\")\n",
    "print(f\"Fine Tune At: {best_hp.get('fine_tune_at')}\")\n",
    "\n",
    "# Jika Anda ingin melanjutkan pelatihan dengan HP terbaik ini (Fase 2)\n",
    "# Anda harus memuat bobot Phase 1 terbaik dan melatih ulang dengan HP yang ditemukan.\n",
    "\n",
    "# model = best_model\n",
    "# print(\"Model sekarang menggunakan HP terbaik dari Random Search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa0abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9a: Generating Comparison Table for All Random Search Trials...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_model_for_tuning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 9a: Generating Comparison Table for All Random Search Trials...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# --- Ambil semua trial yang telah dijalankan (misalnya 30 trial) ---\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Buat ulang objek Tuner, menunjuk ke direktori yang SAMA\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Kita harus menggunakan fungsi builder yang sama, meskipun tidak dilatih\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39m\u001b[43mbuild_model_for_tuning\u001b[49m, \n\u001b[0;32m      8\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mae\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      9\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     10\u001b[0m     executions_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     11\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maflatoxin_tuning_results\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Pastikan ini nama folder yang benar\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResNet_Aflatoxin_Reg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Gunakan max_trials dari tuner (30) atau angka yang cukup besar\u001b[39;00m\n\u001b[0;32m     15\u001b[0m all_trials \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_trials(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_model_for_tuning' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Step 9a: Generating Comparison Table for All Random Search Trials...\")\n",
    "\n",
    "# --- Ambil semua trial yang telah dijalankan (misalnya 30 trial) ---\n",
    "# Gunakan max_trials dari tuner (30) atau angka yang cukup besar\n",
    "all_trials = tuner.get_best_trials(num_trials=30) \n",
    "trial_data = []\n",
    "\n",
    "# Ekstrak Hyperparameter dan Skor dari setiap trial\n",
    "for trial in all_trials:\n",
    "    trial_id = trial.trial_id\n",
    "    hp = trial.hyperparameters.values\n",
    "    \n",
    "    # Ambil skor terbaik (MAE) dari trial tersebut\n",
    "    # Keras Tuner menyimpan skor final dari EarlyStopping/best epoch\n",
    "    score = trial.best_trial.score \n",
    "    \n",
    "    # Kumpulkan data ke dalam dictionary\n",
    "    trial_record = {\n",
    "        'Trial ID': trial_id,\n",
    "        'val_MAE': score,\n",
    "        'LR (Phase 2)': hp.get('learning_rate'),\n",
    "        'Optimizer': hp.get('optimizer'),\n",
    "        'Units L1': hp.get('units_l1'),\n",
    "        'Dropout': hp.get('dropout_rate'),\n",
    "        'Fine Tune At': hp.get('fine_tune_at')\n",
    "    }\n",
    "    trial_data.append(trial_record)\n",
    "\n",
    "# --- Konversi ke DataFrame dan Tampilkan ---\n",
    "df_results = pd.DataFrame(trial_data)\n",
    "\n",
    "# Urutkan berdasarkan val_MAE (Semakin kecil semakin baik)\n",
    "df_results = df_results.sort_values(by='val_MAE', ascending=True)\n",
    "\n",
    "# Format kolom numerik untuk tampilan yang lebih rapi\n",
    "df_results['val_MAE'] = df_results['val_MAE'].round(4)\n",
    "df_results['LR (Phase 2)'] = df_results['LR (Phase 2)'].apply(lambda x: f'{x:.1e}')\n",
    "df_results['Dropout'] = df_results['Dropout'].round(2)\n",
    "\n",
    "\n",
    "print(\"\\n--- PERBANDINGAN HASIL SEMUA TRIAL RANDOM SEARCH (TERBAIK KE TERBURUK) ---\")\n",
    "# Reset index untuk tampilan ranking\n",
    "df_results = df_results.reset_index(drop=True)\n",
    "df_results.index.name = 'Rank'\n",
    "df_results.index += 1 # Mulai dari Rank 1\n",
    "\n",
    "# Tampilkan tabel\n",
    "print(df_results.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee15a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
