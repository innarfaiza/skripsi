{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM SEARCH HYPERPARAMETER OPTIMIZATION\n",
    "## Model Klasifikasi Aflatoksin dengan ResNet-18 (Custom)\n",
    "\n",
    "### Referensi Ilmiah:\n",
    "1. **Bergstra & Bengio (2012)** - \"Random Search for Hyper-Parameter Optimization\"\n",
    "2. **Yang & Shami (2020)** - \"On Hyperparameter Optimization of Machine Learning Algorithms\"\n",
    "3. **Yu & Zhu (2020)** - \"Hyper-Parameter Optimization: A Review of Algorithms and Applications\"\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è CATATAN PENTING\n",
    "\n",
    "**ResNet18 TIDAK tersedia di `tensorflow.keras.applications`!**\n",
    "\n",
    "Notebook ini menggunakan **Custom ResNet18** yang dibangun dengan Keras layers.\n",
    "\n",
    "| Model | Parameter | Pre-trained? |\n",
    "|-------|-----------|-------------|\n",
    "| ResNet18 (Custom) | ~11.7 juta | ‚ùå Tidak |\n",
    "| ResNet50 (Keras) | ~25.6 juta | ‚úÖ Ya |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, Activation, MaxPooling2D,\n",
    "    GlobalAveragePooling2D, Dense, Dropout, Add, Input\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPRODUCIBILITY\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KONFIGURASI\n",
    "BASE_DIR = pathlib.Path(\".\")\n",
    "DATA_DIR = BASE_DIR / 'dataset_final'\n",
    "RESULTS_DIR = BASE_DIR / 'random_search_results_resnet18'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Jumlah trial random search\n",
    "N_TRIALS = 20\n",
    "\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEARCH SPACE DEFINITION\n",
    "\n",
    "Search space disesuaikan untuk ResNet18 yang lebih kecil:\n",
    "\n",
    "| Parameter | Range ResNet50 | Range ResNet18 | Alasan |\n",
    "|-----------|----------------|----------------|--------|\n",
    "| LR Phase 1 | [1e-5, 1e-3] | [1e-4, 1e-2] | Model dari scratch butuh LR lebih tinggi |\n",
    "| LR Phase 2 | [1e-6, 1e-4] | [1e-5, 1e-3] | Fine-tune lebih agresif |\n",
    "| Fine-tune Layer | [100, 170] | [20, 50] | ResNet18 hanya ~60 layers |\n",
    "| Dense Units | [128, 1024] | [64, 512] | Model lebih kecil, head lebih sederhana |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEARCH SPACE untuk ResNet18\n",
    "SEARCH_SPACE = {\n",
    "    # Learning rate lebih tinggi karena training from scratch\n",
    "    'lrp1': [1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'lrp2': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3],\n",
    "    \n",
    "    # Layer options - lebih sederhana untuk model kecil\n",
    "    'layer_options': [\n",
    "        [128],               # 1 layer: 128\n",
    "        [256],               # 1 layer: 256\n",
    "        [256, 128],          # 2 layers: 256 ‚Üí 128\n",
    "        [512, 256],          # 2 layers: 512 ‚Üí 256\n",
    "        [256, 128, 64],      # 3 layers: 256 ‚Üí 128 ‚Üí 64\n",
    "    ],\n",
    "    \n",
    "    'dropout_rate': [0.3, 0.4, 0.5, 0.6],\n",
    "    'optimizer': ['adam', 'adamw', 'sgd'],\n",
    "    \n",
    "    # Epochs lebih banyak karena training from scratch\n",
    "    'epochs_p1': [30, 40, 50, 60],\n",
    "    'epochs_p2': [30, 40, 50, 60],\n",
    "    \n",
    "    # Fine-tune layer - ResNet18 punya ~60 layers\n",
    "    'fine_tune_at': [20, 30, 40, 50],\n",
    "}\n",
    "\n",
    "print(\"Search Space untuk ResNet18:\")\n",
    "for key, values in SEARCH_SPACE.items():\n",
    "    print(f\"  {key}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM RESNET18 IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "def basic_block(x, filters, stride=1, downsample=None, name=None):\n",
    "    \"\"\"\n",
    "    Basic Block untuk ResNet18/34\n",
    "    x ‚Üí Conv3x3 ‚Üí BN ‚Üí ReLU ‚Üí Conv3x3 ‚Üí BN ‚Üí Add(x) ‚Üí ReLU\n",
    "    \"\"\"\n",
    "    identity = x\n",
    "    \n",
    "    # First conv\n",
    "    out = Conv2D(filters, 3, strides=stride, padding='same', \n",
    "                 use_bias=False, name=f'{name}_conv1')(x)\n",
    "    out = BatchNormalization(name=f'{name}_bn1')(out)\n",
    "    out = Activation('relu', name=f'{name}_relu1')(out)\n",
    "    \n",
    "    # Second conv\n",
    "    out = Conv2D(filters, 3, strides=1, padding='same', \n",
    "                 use_bias=False, name=f'{name}_conv2')(out)\n",
    "    out = BatchNormalization(name=f'{name}_bn2')(out)\n",
    "    \n",
    "    # Shortcut connection\n",
    "    if downsample is not None:\n",
    "        identity = downsample(x)\n",
    "    \n",
    "    out = Add(name=f'{name}_add')([out, identity])\n",
    "    out = Activation('relu', name=f'{name}_relu2')(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def make_layer(x, filters, blocks, stride=1, name=None):\n",
    "    \"\"\"\n",
    "    Membuat layer yang terdiri dari beberapa basic blocks\n",
    "    \"\"\"\n",
    "    downsample = None\n",
    "    \n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        downsample = Sequential([\n",
    "            Conv2D(filters, 1, strides=stride, use_bias=False),\n",
    "            BatchNormalization()\n",
    "        ], name=f'{name}_downsample')\n",
    "    \n",
    "    x = basic_block(x, filters, stride, downsample, name=f'{name}_block1')\n",
    "    \n",
    "    for i in range(1, blocks):\n",
    "        x = basic_block(x, filters, name=f'{name}_block{i+1}')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_resnet18_base(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Membangun base ResNet18 (tanpa classification head)\n",
    "    \n",
    "    Struktur:\n",
    "    - Conv1: 7x7, 64 filters, stride 2\n",
    "    - MaxPool: 3x3, stride 2  \n",
    "    - Layer1: 2 basic blocks, 64 filters\n",
    "    - Layer2: 2 basic blocks, 128 filters, stride 2\n",
    "    - Layer3: 2 basic blocks, 256 filters, stride 2\n",
    "    - Layer4: 2 basic blocks, 512 filters, stride 2\n",
    "    - Global Average Pooling\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Initial convolution\n",
    "    x = Conv2D(64, 7, strides=2, padding='same', use_bias=False, name='conv1')(inputs)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    x = MaxPooling2D(3, strides=2, padding='same', name='maxpool')(x)\n",
    "    \n",
    "    # Residual layers\n",
    "    x = make_layer(x, 64, 2, stride=1, name='layer1')\n",
    "    x = make_layer(x, 128, 2, stride=2, name='layer2')\n",
    "    x = make_layer(x, 256, 2, stride=2, name='layer3')\n",
    "    x = make_layer(x, 512, 2, stride=2, name='layer4')\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = GlobalAveragePooling2D(name='avgpool')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x, name='resnet18_base')\n",
    "    return model\n",
    "\n",
    "print(\"Custom ResNet18 builder defined.\")\n",
    "print(\"\\nStruktur ResNet18:\")\n",
    "print(\"  - Conv1: 7x7, 64 filters\")\n",
    "print(\"  - Layer1: 2 basic blocks, 64 filters\")\n",
    "print(\"  - Layer2: 2 basic blocks, 128 filters\")\n",
    "print(\"  - Layer3: 2 basic blocks, 256 filters\")\n",
    "print(\"  - Layer4: 2 basic blocks, 512 filters\")\n",
    "print(\"  - Global Average Pooling\")\n",
    "\n",
    "# Test build\n",
    "test_model = build_resnet18_base()\n",
    "print(f\"\\nTotal layers in base model: {len(test_model.layers)}\")\n",
    "print(f\"Total parameters: {test_model.count_params():,}\")\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def sample_hyperparameters(search_space, seed=None):\n",
    "    \"\"\"Random sampling dari search space.\"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    params = {}\n",
    "    for key, values in search_space.items():\n",
    "        params[key] = random.choice(values)\n",
    "    \n",
    "    # Constraint: lrp2 <= lrp1\n",
    "    if params['lrp2'] > params['lrp1']:\n",
    "        params['lrp2'] = params['lrp1'] / 10\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_optimizer(name, learning_rate):\n",
    "    \"\"\"Create optimizer instance.\"\"\"\n",
    "    if name == 'adam':\n",
    "        return keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif name == 'adamw':\n",
    "        return keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n",
    "    elif name == 'sgd':\n",
    "        return keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif name == 'rmsprop':\n",
    "        return keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    return keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Test sampling\n",
    "print(\"Contoh sampling:\")\n",
    "for i in range(5):\n",
    "    p = sample_hyperparameters(SEARCH_SPACE, seed=100+i)\n",
    "    arch = \" ‚Üí \".join(map(str, p['layer_options']))\n",
    "    print(f\"  Sample {i+1}: [{arch}], LR1={p['lrp1']}, Opt={p['optimizer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASETS\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / 'train', labels=\"inferred\", label_mode=\"int\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / 'val', labels=\"inferred\", label_mode=\"int\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR / 'test', labels=\"inferred\", label_mode=\"int\",\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH), batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Augmentation & Preprocessing untuk ResNet18\n",
    "# Menggunakan normalisasi [0, 1] karena tidak ada pre-trained weights\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.028),  # ~1.6 derajat\n",
    "])\n",
    "\n",
    "def preprocess_train(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = data_augmentation(images, training=True)\n",
    "    images = images / 255.0  # Normalize ke [0, 1]\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_val(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = images / 255.0  # Normalize ke [0, 1]\n",
    "    return images, labels\n",
    "\n",
    "# Apply preprocessing\n",
    "train_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess_val, num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess_val, num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"\\n‚úì Datasets loaded and preprocessed\")\n",
    "print(\"  - Normalization: [0, 1]\")\n",
    "print(\"  - Augmentation: RandomFlip, RandomRotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD MODEL FUNCTION\n",
    "def build_model(params):\n",
    "    \"\"\"Build ResNet18 model dengan hyperparameters.\"\"\"\n",
    "    \n",
    "    # Build base ResNet18\n",
    "    base_model = build_resnet18_base(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    base_model.trainable = False  # Freeze untuk Phase 1\n",
    "    \n",
    "    # Build full model dengan classification head\n",
    "    inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Dense layers berdasarkan layer_options\n",
    "    layer_units = params['layer_options']\n",
    "    \n",
    "    for i, units in enumerate(layer_units):\n",
    "        x = layers.Dense(units, activation='relu', name=f'dense_{i+1}')(x)\n",
    "        # Dropout setelah setiap layer kecuali layer terakhir\n",
    "        if i < len(layer_units) - 1:\n",
    "            x = layers.Dropout(params['dropout_rate'], name=f'dropout_{i+1}')(x)\n",
    "    \n",
    "    # Jika hanya 1 layer, tambah dropout sebelum output\n",
    "    if len(layer_units) == 1:\n",
    "        x = layers.Dropout(params['dropout_rate'], name='dropout_1')(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='ResNet18_Aflatoxin')\n",
    "    return model, base_model\n",
    "\n",
    "# Test build\n",
    "test_params = sample_hyperparameters(SEARCH_SPACE, seed=42)\n",
    "test_model, test_base = build_model(test_params)\n",
    "print(f\"Test model built successfully!\")\n",
    "print(f\"  Total parameters: {test_model.count_params():,}\")\n",
    "print(f\"  Base model layers: {len(test_base.layers)}\")\n",
    "del test_model, test_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SINGLE TRIAL\n",
    "def train_trial(params, trial_id):\n",
    "    \"\"\"Train single trial.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAL {trial_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Format architecture string\n",
    "    layer_units = params['layer_options']\n",
    "    arch_str = \" ‚Üí \".join(map(str, layer_units))\n",
    "    \n",
    "    print(f\"Architecture: [{arch_str}] ({len(layer_units)} layers)\")\n",
    "    print(f\"LR_P1={params['lrp1']}, LR_P2={params['lrp2']}, Dropout={params['dropout_rate']}\")\n",
    "    print(f\"Optimizer={params['optimizer']}, Fine-tune at={params['fine_tune_at']}\")\n",
    "    print(f\"Epochs P1={params['epochs_p1']}, Epochs P2={params['epochs_p2']}\")\n",
    "    \n",
    "    model, base_model = build_model(params)\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy', \n",
    "            patience=15,  # Lebih sabar karena training from scratch\n",
    "            mode='max', \n",
    "            restore_best_weights=True, \n",
    "            verbose=0\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=8, \n",
    "            min_lr=1e-8, \n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Phase 1: Frozen backbone\n",
    "    print(f\"\\n[Phase 1] Training head only, Epochs={params['epochs_p1']}\")\n",
    "    model.compile(\n",
    "        optimizer=get_optimizer(params['optimizer'], params['lrp1']),\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history_p1 = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds, \n",
    "        epochs=params['epochs_p1'], \n",
    "        callbacks=callbacks, \n",
    "        verbose=0\n",
    "    )\n",
    "    p1_acc = max(history_p1.history['val_accuracy'])\n",
    "    p1_epochs_run = len(history_p1.history['loss'])\n",
    "    print(f\"  Phase 1 Val Acc: {p1_acc:.4f} (ran {p1_epochs_run} epochs)\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning\n",
    "    print(f\"\\n[Phase 2] Fine-tuning from layer {params['fine_tune_at']}, Epochs={params['epochs_p2']}\")\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze layers sebelum fine_tune_at\n",
    "    for layer in base_model.layers[:params['fine_tune_at']]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Count trainable layers\n",
    "    trainable_count = sum([1 for layer in base_model.layers if layer.trainable])\n",
    "    print(f\"  Trainable layers in base: {trainable_count}\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=get_optimizer(params['optimizer'], params['lrp2']),\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    initial_epoch = p1_epochs_run\n",
    "    history_p2 = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds,\n",
    "        initial_epoch=initial_epoch, \n",
    "        epochs=initial_epoch + params['epochs_p2'],\n",
    "        callbacks=callbacks, \n",
    "        verbose=0\n",
    "    )\n",
    "    p2_acc = max(history_p2.history['val_accuracy'])\n",
    "    p2_epochs_run = len(history_p2.history['loss'])\n",
    "    print(f\"  Phase 2 Val Acc: {p2_acc:.4f} (ran {p2_epochs_run} epochs)\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)\n",
    "    y_pred = np.argmax(model.predict(test_ds, verbose=0), axis=1)\n",
    "    y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\n[Result] Test Accuracy: {test_accuracy:.4f}, F1 Macro: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'trial_id': trial_id,\n",
    "        'architecture': arch_str,\n",
    "        'params': {\n",
    "            'lrp1': params['lrp1'],\n",
    "            'lrp2': params['lrp2'],\n",
    "            'layer_options': params['layer_options'],\n",
    "            'dropout_rate': params['dropout_rate'],\n",
    "            'optimizer': params['optimizer'],\n",
    "            'epochs_p1': params['epochs_p1'],\n",
    "            'epochs_p2': params['epochs_p2'],\n",
    "            'fine_tune_at': params['fine_tune_at'],\n",
    "        },\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_loss': float(test_loss),\n",
    "        'f1_macro': float(f1),\n",
    "        'val_acc_p1': float(p1_acc),\n",
    "        'val_acc_p2': float(p2_acc),\n",
    "        'epochs_ran_p1': p1_epochs_run,\n",
    "        'epochs_ran_p2': p2_epochs_run,\n",
    "    }, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN RANDOM SEARCH\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING RANDOM SEARCH - RESNET18\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total trials: {N_TRIALS}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "all_results = []\n",
    "best_result = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for trial_id in range(1, N_TRIALS + 1):\n",
    "    trial_start = datetime.now()\n",
    "    \n",
    "    # Sample hyperparameters\n",
    "    params = sample_hyperparameters(SEARCH_SPACE, seed=42 + trial_id)\n",
    "    \n",
    "    try:\n",
    "        result, model = train_trial(params, trial_id)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save trial result\n",
    "        with open(RESULTS_DIR / f'trial_{trial_id:03d}.json', 'w') as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        \n",
    "        # Check if best\n",
    "        if result['test_accuracy'] > best_accuracy:\n",
    "            best_accuracy = result['test_accuracy']\n",
    "            best_result = result\n",
    "            model.save(RESULTS_DIR / 'best_model_resnet18.keras')\n",
    "            with open(RESULTS_DIR / 'best_params_resnet18.json', 'w') as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            print(f\"\\nüèÜ NEW BEST! Accuracy: {best_accuracy:.4f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Trial {trial_id} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    trial_duration = datetime.now() - trial_start\n",
    "    print(f\"\\nProgress: {trial_id}/{N_TRIALS} | Best so far: {best_accuracy:.4f} | Trial time: {trial_duration}\")\n",
    "\n",
    "# Save all results\n",
    "with open(RESULTS_DIR / 'search_history_resnet18.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "total_duration = datetime.now() - start_time\n",
    "print(f\"\\n\\nTotal search time: {total_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM SEARCH COMPLETED - RESNET18\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTotal trials completed: {len(all_results)}\")\n",
    "print(f\"Best Test Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "if best_result:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ BEST HYPERPARAMETERS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"  Architecture: [{best_result['architecture']}]\")\n",
    "    print(f\"  LR Phase 1: {best_result['params']['lrp1']}\")\n",
    "    print(f\"  LR Phase 2: {best_result['params']['lrp2']}\")\n",
    "    print(f\"  Dropout: {best_result['params']['dropout_rate']}\")\n",
    "    print(f\"  Optimizer: {best_result['params']['optimizer']}\")\n",
    "    print(f\"  Epochs P1: {best_result['params']['epochs_p1']} (ran {best_result['epochs_ran_p1']})\")\n",
    "    print(f\"  Epochs P2: {best_result['params']['epochs_p2']} (ran {best_result['epochs_ran_p2']})\")\n",
    "    print(f\"  Fine-tune at: {best_result['params']['fine_tune_at']}\")\n",
    "    print(f\"\\n  Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "    print(f\"  F1 Macro: {best_result['f1_macro']:.4f}\")\n",
    "    print(f\"  Val Acc P1: {best_result['val_acc_p1']:.4f}\")\n",
    "    print(f\"  Val Acc P2: {best_result['val_acc_p2']:.4f}\")\n",
    "\n",
    "# Sorted results table\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä TOP 10 TRIALS (sorted by accuracy)\")\n",
    "print(\"=\"*50)\n",
    "sorted_results = sorted(all_results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "\n",
    "print(f\"{'Trial':<8} {'Accuracy':<10} {'F1':<8} {'Architecture':<20} {'Optimizer':<10}\")\n",
    "print(\"-\"*60)\n",
    "for r in sorted_results[:10]:\n",
    "    print(f\"{r['trial_id']:<8} {r['test_accuracy']:.4f}     {r['f1_macro']:.4f}   [{r['architecture']}]  {r['params']['optimizer']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {RESULTS_DIR}\")\n",
    "print(f\"   - best_model_resnet18.keras\")\n",
    "print(f\"   - best_params_resnet18.json\")\n",
    "print(f\"   - search_history_resnet18.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: VISUALIZE RESULTS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Accuracy distribution\n",
    "    accuracies = [r['test_accuracy'] for r in all_results]\n",
    "    axes[0, 0].hist(accuracies, bins=15, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(best_accuracy, color='red', linestyle='--', label=f'Best: {best_accuracy:.4f}')\n",
    "    axes[0, 0].set_xlabel('Test Accuracy')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Distribution of Test Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Accuracy vs Trial\n",
    "    trial_ids = [r['trial_id'] for r in all_results]\n",
    "    axes[0, 1].plot(trial_ids, accuracies, 'b.-', alpha=0.7)\n",
    "    axes[0, 1].axhline(best_accuracy, color='red', linestyle='--', label=f'Best: {best_accuracy:.4f}')\n",
    "    axes[0, 1].set_xlabel('Trial ID')\n",
    "    axes[0, 1].set_ylabel('Test Accuracy')\n",
    "    axes[0, 1].set_title('Test Accuracy per Trial')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Accuracy by Optimizer\n",
    "    optimizers = list(set([r['params']['optimizer'] for r in all_results]))\n",
    "    opt_accs = {opt: [r['test_accuracy'] for r in all_results if r['params']['optimizer'] == opt] for opt in optimizers}\n",
    "    axes[1, 0].boxplot([opt_accs[opt] for opt in optimizers], labels=optimizers)\n",
    "    axes[1, 0].set_xlabel('Optimizer')\n",
    "    axes[1, 0].set_ylabel('Test Accuracy')\n",
    "    axes[1, 0].set_title('Accuracy by Optimizer')\n",
    "    \n",
    "    # 4. Accuracy by Architecture depth\n",
    "    depths = [len(r['params']['layer_options']) for r in all_results]\n",
    "    depth_accs = {d: [r['test_accuracy'] for r in all_results if len(r['params']['layer_options']) == d] \n",
    "                  for d in set(depths)}\n",
    "    axes[1, 1].boxplot([depth_accs[d] for d in sorted(depth_accs.keys())], \n",
    "                       labels=[f\"{d} layers\" for d in sorted(depth_accs.keys())])\n",
    "    axes[1, 1].set_xlabel('Architecture Depth')\n",
    "    axes[1, 1].set_ylabel('Test Accuracy')\n",
    "    axes[1, 1].set_title('Accuracy by Head Depth')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'search_analysis_resnet18.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Analysis plot saved to: {RESULTS_DIR / 'search_analysis_resnet18.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Catatan\n",
    "\n",
    "### Perbedaan dengan Random Search ResNet50:\n",
    "\n",
    "| Aspek | ResNet50 | ResNet18 |\n",
    "|-------|----------|----------|\n",
    "| Pre-trained | ‚úÖ ImageNet | ‚ùå From scratch |\n",
    "| LR Range | [1e-5, 1e-3] | [1e-4, 1e-2] |\n",
    "| Fine-tune Layer | [100, 170] | [20, 50] |\n",
    "| Epochs | Lebih sedikit | Lebih banyak |\n",
    "| Patience | 10 | 15 |\n",
    "\n",
    "### Jika hasil kurang memuaskan:\n",
    "\n",
    "1. Gunakan model dengan **pre-trained weights**:\n",
    "   - EfficientNet-B0\n",
    "   - MobileNetV2\n",
    "   - DenseNet121\n",
    "\n",
    "2. Coba **longer training**:\n",
    "   - Tambah epochs\n",
    "   - Kurangi LR decay\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
